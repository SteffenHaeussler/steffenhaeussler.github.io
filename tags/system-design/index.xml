<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>System Design on About me</title>
    <link>http://localhost:1313/tags/system-design/</link>
    <description>Recent content in System Design on About me</description>
    <generator>Hugo -- 0.147.5</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 10:30:58 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/system-design/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building Agents &amp; LLM Workflows</title>
      <link>http://localhost:1313/posts/2025-07-30-hackerroom/hackerroom/</link>
      <pubDate>Wed, 30 Jul 2025 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2025-07-30-hackerroom/hackerroom/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;Over the last three months, I&amp;rsquo;ve been working on Agents and LLM workflows. I had the opportunity to do this as part of a &lt;a href=&#34;https://www.merantix-aicampus.com/hacker-room&#34;&gt;residency program&lt;/a&gt; at &lt;a href=&#34;https://www.merantix-aicampus.com/&#34;&gt;Merantix&lt;/a&gt;.
I&amp;rsquo;m deeply thankful for the opportunity and would highly recommend the program to others. Being surrounded by people with shared goals was incredibly motivating and inspiring.
I already miss the discussions and the knowledge exchange. Unlike typical work environments, here the participants were truly aligned, and everyone learned from each other.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Evaluation of RAG Systems</title>
      <link>http://localhost:1313/posts/2025-04-30-metrics/metrics/</link>
      <pubDate>Wed, 30 Apr 2025 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2025-04-30-metrics/metrics/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;The implementation of this article is &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/evaluation-of-rag-systems&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;RAGs are complex systems. This is obvious, when you try to evaluate them. There are multiple aspects, which need to be checked. Here, I try to look into different approaches to get a better understanding and problems, when facing RAG systems. RAG system evaluation involves two distinct parts: retrieval and generation part. For retrieval, context relevance and noise robustness are key factors in assessing quality, while for generation part, key factors like answer faithfulness, answer relevance, negative rejection, information integration, and counterfactual robustness are important (&lt;a href=&#34;https://arxiv.org/pdf/2312.10997#page=14&#34;&gt;Gao et al. 2024&lt;/a&gt;).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Get embeddings for multiple data sources</title>
      <link>http://localhost:1313/posts/2025-01-02-embeddings/embeddings/</link>
      <pubDate>Thu, 02 Jan 2025 10:30:58 +0100</pubDate>
      <guid>https://steffenhaeussler.github.io/posts/2025-01-02-embeddings/embeddings/</guid>
      <description>&lt;p&gt;DISCLAIMER: This is wrong. Please ignore this post.&lt;/p&gt;
&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;Following my first short post about &lt;a href=&#34;https://steffenhaeussler.github.io/posts/2024-12-27-overview/rag_overview/&#34;&gt;RAGs&lt;/a&gt;, I would like to provide a brief overview about embeddings, which are used to find similiar objects in a vector database. To better understand how various transformer models handle different input data types, I created this &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/get-embeddings-for-multiple-data-types&#34;&gt;notebook&lt;/a&gt;. I explore therefor, text, image, audio and video data.&lt;/p&gt;
&lt;p&gt;I’ve chosen to skip the more traditional text embeddings (TF-IDF, Word2Vec or GloVe), because there are already very good tutorials available. Additionally, I plan to discuss the training of embedding models in a separate blog post. For this post, I use mostly pretrained classification models, where I use the last layer before the prediction head as embedding.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Overview of RAG (Retrieval-Augmented Generation) systems</title>
      <link>http://localhost:1313/posts/2024-12-27-overview/rag_overview/</link>
      <pubDate>Fri, 27 Dec 2024 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2024-12-27-overview/rag_overview/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;It’s been a while since my last post, mostly because of my own laziness. Over the past year, I’ve been working on several projects, one of which is a small RAG (Retrieval-Augmented Generation) system. I implemented it to combine external knowledge (in this case internal safety documents) with a large language model (LLM). This approach allows the use of data that the LLM wasn&amp;rsquo;t trained on and also helps reduce hallucinations.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
