<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Deep Learning on About me</title>
    <link>https://steffenhaeussler.github.io/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on About me</description>
    <generator>Hugo -- 0.147.5</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Dec 2023 10:30:58 +0100</lastBuildDate>
    <atom:link href="https://steffenhaeussler.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deep Learning model explainability</title>
      <link>https://steffenhaeussler.github.io/posts/2023-12-08-dl-explainability/dl_model_explainability/</link>
      <pubDate>Fri, 08 Dec 2023 10:30:58 +0100</pubDate>
      <guid>https://steffenhaeussler.github.io/posts/2023-12-08-dl-explainability/dl_model_explainability/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://steffenhaeussler.github.io/posts/model_explainability/&#34;&gt;In my first post&lt;/a&gt;, I looked into the explainability of classical machine learning models. As a next step, I&amp;rsquo;m interested in the explainability of neural networks.  Model explainability is easy for simple models (linear regression, decision trees), and some tools exist for more complex algorithms (ensemble trees). Therefore, I highly recommend the book &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/&#34;&gt;Interpretable Machine Learning by Christoph Molnar&lt;/a&gt; for a deeper theoretical understanding. All different approaches for model explanability are shown with a PyTorch model &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/mohs-hardness-model-explainer-for-neuralnets/notebook&#34;&gt;in this kaggle notebook&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning about time-series analysis</title>
      <link>https://steffenhaeussler.github.io/posts/2023-08-15-timeseries/time_series/</link>
      <pubDate>Tue, 15 Aug 2023 00:30:58 +0100</pubDate>
      <guid>https://steffenhaeussler.github.io/posts/2023-08-15-timeseries/time_series/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;Recently, I had to work on a simple time-series analysis. I performed poorly since I never worked with time-series before.
I believe in a deterministic world, and in general, I prefer to find the causality of a specific data behavior prior to a simple way of empiristic modeling. However, I understand the need for time-series analysis as not enough data available, the underlying processes understood, the complexity bearable, or the time/need for a proper process understanding. The goal is to make a prediction based on the previously observed observations. In a traditional sense (Arima), you look at the trend, seasonality, and cycles - in the more modern way, you throw the data into a model architecture (deep learning). In this context, I should mention the famous paper &lt;a href=&#34;https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.pdf&#34;&gt;Statistical Modeling: The Two Cultures&lt;/a&gt;, while I prefer to use algorithmic models and treat the data mechanism as unknown. I would add that the underlying data mechanism is deterministic, and we should use collected data to get improved models. Anyway, let&amp;rsquo;s use the many resources in the time-series field to get better in this field.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
