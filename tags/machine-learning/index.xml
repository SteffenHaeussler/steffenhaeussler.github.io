<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine Learning on About me</title>
    <link>http://localhost:1313/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on About me</description>
    <generator>Hugo -- 0.147.5</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Jan 2026 10:30:58 +0100</lastBuildDate>
    <atom:link href="https://steffenhaeussler.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Training an embedding model</title>
      <link>https://steffenhaeussler.github.io/posts/2026-01-09-contrastive/contrastive/</link>
      <pubDate>Fri, 09 Jan 2026 10:30:58 +0100</pubDate>
      <guid>https://steffenhaeussler.github.io/posts/2026-01-09-contrastive/contrastive/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;Based on my &lt;a href=&#34;https://steffenhaeussler.github.io/posts/2025-01-02-embeddings/embeddings/&#34;&gt;previous post&lt;/a&gt;, I need to write a correction. I was wrong.&lt;/p&gt;
&lt;p&gt;I previously suggested using embeddings directly from pre-trained models. That turns out to be a bad idea, because the training objectives are fundamentally different. As an alternative, I have now explored training an embedding model using Contrastive Learning.&lt;/p&gt;
&lt;p&gt;Please look at my &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/contrastive-learning&#34;&gt;notebook&lt;/a&gt; for the full code and details.&lt;/p&gt;
&lt;p&gt;The main reason you should not use raw embeddings from pre-trained models is the anisotropy problem. The paper &lt;a href=&#34;https://arxiv.org/pdf/1909.00512&#34;&gt;How Contextual are Contextualized Word Representations? from Ethayarajh et al. 2019&lt;/a&gt; shows that the embeddings are clustered in a narrow cone of the vector space, making them almost useless for differentiation. So, if you calculate the cosine similarity between two completely different sentences, the result will be around 0.80.&lt;/p&gt;</description>
    </item>
    <item>
      <title>This year&#39;s recap v2</title>
      <link>http://localhost:1313/posts/2025-12-29-recap/recapv2/</link>
      <pubDate>Mon, 29 Dec 2025 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2025-12-29-recap/recapv2/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;I decided to add another post to this year&amp;rsquo;s recap. After I described my three key learnings from my failed startup endeavor, I wasn&amp;rsquo;t feeling finished or satisfied. Overall I&amp;rsquo;m not satisfied with what I learned this year. It feels very shallow without any hard skills behind it. In every year of my life, I learned a lot, independent of the later usefulness, but at least I can say, that I moved forward. This year, I don&amp;rsquo;t feel like this. As Confucius said: &amp;ldquo;By three methods we may learn wisdom: First, by reflection, which is noblest; Second, by imitation, which is easiest; and third by experience, which is the bitterest.&amp;rdquo; My learnings weren&amp;rsquo;t helpful at all and they came from experience.
To sum it up, the ratio of effort and outcome was very low and I definitely can do better.&lt;/p&gt;</description>
    </item>
    <item>
      <title>This year&#39;s recap</title>
      <link>http://localhost:1313/posts/2025-12-12-recap/recap/</link>
      <pubDate>Fri, 12 Dec 2025 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2025-12-12-recap/recap/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;This year was an interesting learning experience. I tried to commercialize a product, which in the end was much harder than expected.
As a short summary, the first attempt failed, because I couldn&amp;rsquo;t figure out how to sell the product. The pivot failed, because multiple big corporates released a similar product. Also the goal was to get 1-2 customers to build a MVP before looking for funding and so on. So I never incorporated the idea into a company. Basically, I failed already on the first meter, but this saved a lot of time. So I only invested 7
months. ü•≤&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building Agents &amp; LLM Workflows</title>
      <link>http://localhost:1313/posts/2025-07-30-hackerroom/hackerroom/</link>
      <pubDate>Wed, 30 Jul 2025 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2025-07-30-hackerroom/hackerroom/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;Over the last three months, I&amp;rsquo;ve been working on Agents and LLM workflows. I had the opportunity to do this as part of a &lt;a href=&#34;https://www.merantix-aicampus.com/hacker-room&#34;&gt;residency program&lt;/a&gt; at &lt;a href=&#34;https://www.merantix-aicampus.com/&#34;&gt;Merantix&lt;/a&gt;.
I&amp;rsquo;m deeply thankful for the opportunity and would highly recommend the program to others. Being surrounded by people with shared goals was incredibly motivating and inspiring.
I already miss the discussions and the knowledge exchange. Unlike typical work environments, here the participants were truly aligned, and everyone learned from each other.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Evaluation of RAG Systems</title>
      <link>http://localhost:1313/posts/2025-04-30-metrics/metrics/</link>
      <pubDate>Wed, 30 Apr 2025 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2025-04-30-metrics/metrics/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;The implementation of this article is &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/evaluation-of-rag-systems&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;RAGs are complex systems. This is obvious, when you try to evaluate them. There are multiple aspects, which need to be checked. Here, I try to look into different approaches to get a better understanding and problems, when facing RAG systems. RAG system evaluation involves two distinct parts: retrieval and generation part. For retrieval, context relevance and noise robustness are key factors in assessing quality, while for generation part, key factors like answer faithfulness, answer relevance, negative rejection, information integration, and counterfactual robustness are important (&lt;a href=&#34;https://arxiv.org/pdf/2312.10997#page=14&#34;&gt;Gao et al. 2024&lt;/a&gt;).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding approximate nearest neighbor algorithm</title>
      <link>http://localhost:1313/posts/2025-04-19-ann/ann/</link>
      <pubDate>Sat, 19 Apr 2025 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2025-04-19-ann/ann/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;This post is about the approximate nearest neighbor (ANN) algorithm. The code for this post is &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/understanding-approx-nearest-neighbor-algorithm&#34;&gt;here&lt;/a&gt;, where I provide an example of using a framework and a python implementation. Most python implementation were written with the help of a LLM. I&amp;rsquo;m amazed, how helpful they are for learning new things. I see them like a drunken professor, which with the right approach will be a very helpful tool.&lt;/p&gt;
&lt;p&gt;As a next step in understanding RAGs, I want to have a closer look at approximate nearest neighbor algorithms. Basically, the purpose is to find the closest vector to a query vector in a database. Since I&amp;rsquo;m also interested into the implementation, I follow mostly this amazing blog post. Vector search is the basic component of vector databases and their main purpose. ANN algorithms are looking for a close match instead of an exact match. This loss of accuracy allows an improvement of efficieny, which allows the search through much bigger datasets, high-dimensional data and real-time apllications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Short example of Information Retrieval</title>
      <link>http://localhost:1313/posts/2025-03-10-retrieval/retrieval/</link>
      <pubDate>Mon, 10 Mar 2025 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2025-03-10-retrieval/retrieval/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;Some time ago, I did a small project on information retrieval. I think, it\s a good idea to share it with all its shortcomings.
Here is the &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/examples-for-information-retrieval&#34;&gt;code&lt;/a&gt;. Sadly, the LLM part doesn&amp;rsquo;t work with the quantized model, so I commented it out.
The project is a small information retrieval of a FAQ, where I want to map the correct answer to a question. In my example, it&amp;rsquo;s a 1:1 mapping between question and answer, but it also works with multiple answers.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Get embeddings for multiple data sources</title>
      <link>http://localhost:1313/posts/2025-01-02-embeddings/embeddings/</link>
      <pubDate>Thu, 02 Jan 2025 10:30:58 +0100</pubDate>
      <guid>https://steffenhaeussler.github.io/posts/2025-01-02-embeddings/embeddings/</guid>
      <description>&lt;p&gt;DISCLAIMER: This is wrong. Please ignore this post.&lt;/p&gt;
&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;Following my first short post about &lt;a href=&#34;https://steffenhaeussler.github.io/posts/2024-12-27-overview/rag_overview/&#34;&gt;RAGs&lt;/a&gt;, I would like to provide a brief overview about embeddings, which are used to find similiar objects in a vector database. To better understand how various transformer models handle different input data types, I created this &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/get-embeddings-for-multiple-data-types&#34;&gt;notebook&lt;/a&gt;. I explore therefor, text, image, audio and video data.&lt;/p&gt;
&lt;p&gt;I‚Äôve chosen to skip the more traditional text embeddings (TF-IDF, Word2Vec or GloVe), because there are already very good tutorials available. Additionally, I plan to discuss the training of embedding models in a separate blog post. For this post, I use mostly pretrained classification models, where I use the last layer before the prediction head as embedding.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Overview of RAG (Retrieval-Augmented Generation) systems</title>
      <link>http://localhost:1313/posts/2024-12-27-overview/rag_overview/</link>
      <pubDate>Fri, 27 Dec 2024 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2024-12-27-overview/rag_overview/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;It‚Äôs been a while since my last post, mostly because of my own laziness. Over the past year, I‚Äôve been working on several projects, one of which is a small RAG (Retrieval-Augmented Generation) system. I implemented it to combine external knowledge (in this case internal safety documents) with a large language model (LLM). This approach allows the use of data that the LLM wasn&amp;rsquo;t trained on and also helps reduce hallucinations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep Learning model explainability</title>
      <link>http://localhost:1313/posts/2023-12-08-dl-explainability/dl_model_explainability/</link>
      <pubDate>Fri, 08 Dec 2023 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2023-12-08-dl-explainability/dl_model_explainability/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://steffenhaeussler.github.io/posts/model_explainability/&#34;&gt;In my first post&lt;/a&gt;, I looked into the explainability of classical machine learning models. As a next step, I&amp;rsquo;m interested in the explainability of neural networks.  Model explainability is easy for simple models (linear regression, decision trees), and some tools exist for more complex algorithms (ensemble trees). Therefore, I highly recommend the book &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/&#34;&gt;Interpretable Machine Learning by Christoph Molnar&lt;/a&gt; for a deeper theoretical understanding. All different approaches for model explanability are shown with a PyTorch model &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/mohs-hardness-model-explainer-for-neuralnets/notebook&#34;&gt;in this kaggle notebook&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model explainability</title>
      <link>http://localhost:1313/posts/2023-11-21-explainability/model_explainability/</link>
      <pubDate>Tue, 21 Nov 2023 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2023-11-21-explainability/model_explainability/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;Some months have passed since my last post. Model explainability is easy for simple models (linear regression, decision trees), and some tools exist for more complex algorithms (ensemble trees). I want to dig into the tools to interpret more complex models with this post. Therefore, I highly recommend the book &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/&#34;&gt;Interpretable Machine Learning by Christoph Molnar&lt;/a&gt; for a deeper theoretical understanding. All different approaches for model explanability are shown with a RandomForest model &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/mohs-hardness-model-explainer/notebook&#34;&gt;in this kaggle notebook&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Implementing a Transformer Network from scratch</title>
      <link>http://localhost:1313/posts/2023-08-28-transformer/transformer/</link>
      <pubDate>Mon, 28 Aug 2023 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2023-08-28-transformer/transformer/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;This post is about my implementation of an &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/dna-sequence-classification-part-4&#34;&gt;encoder transformer network from scratch&lt;/a&gt; as a follow-up &lt;a href=&#34;https://steffenhaeussler.github.io/posts/attention_layer/&#34;&gt;of understanding the attention layer&lt;/a&gt; together with the &lt;a href=&#34;https://colab.research.google.com/drive/1sE0B4NOAu9kqUTpuHIp1EYlD5JCAw4Gq?usp=sharing&#34;&gt;colab implementation&lt;/a&gt;. I use a simplified dataset, where I don&amp;rsquo;t expect great results. My approach is building something from scratch to understand it in depth. I faced many challenges during my implementation, so I aligned my code to the &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification&#34;&gt;BertSequenceClassifier from huggingface&lt;/a&gt;. My biggest challenge was to get the network to train. This challenge took me several months of low focus and a proper de- and reconstruction of the architecture. Minor issues were missing skip connections and some data preparation issues.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning about time-series analysis</title>
      <link>http://localhost:1313/posts/2023-08-15-timeseries/time_series/</link>
      <pubDate>Tue, 15 Aug 2023 00:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2023-08-15-timeseries/time_series/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;Recently, I had to work on a simple time-series analysis. I performed poorly since I never worked with time-series before.
I believe in a deterministic world, and in general, I prefer to find the causality of a specific data behavior prior to a simple way of empiristic modeling. However, I understand the need for time-series analysis as not enough data available, the underlying processes understood, the complexity bearable, or the time/need for a proper process understanding. The goal is to make a prediction based on the previously observed observations. In a traditional sense (Arima), you look at the trend, seasonality, and cycles - in the more modern way, you throw the data into a model architecture (deep learning). In this context, I should mention the famous paper &lt;a href=&#34;https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.pdf&#34;&gt;Statistical Modeling: The Two Cultures&lt;/a&gt;, while I prefer to use algorithmic models and treat the data mechanism as unknown. I would add that the underlying data mechanism is deterministic, and we should use collected data to get improved models. Anyway, let&amp;rsquo;s use the many resources in the time-series field to get better in this field.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Training a language model from scratch</title>
      <link>http://localhost:1313/posts/2023-04-21-training/llm/</link>
      <pubDate>Sat, 15 Apr 2023 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2023-04-21-training/llm/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;This post is a short overview over a &lt;a href=&#34;https://www.kaggle.com/code/steffenhaeussler/train-a-language-model-from-scratch/notebook&#34;&gt;work project&lt;/a&gt;, where I trained a language model for invoices. This so-called base model is then fine-tuned for text classification on customer data. Due to data privacy, a non-disclosure agreement, ISO 27001 and SOAP2, I&amp;rsquo;m not allowed to publish any results. Believe me, it works like üöÄ‚ú®ü™ê.&lt;/p&gt;
&lt;p&gt;A language model is trained on large amounts of textual data to understand the patterns and structure of language. The primary goal of a language model is to predict the probability of the next word or sequence of words in a sentence given the previous words.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cookie-cutter Problems</title>
      <link>http://localhost:1313/posts/2023-04-10-cookie/cookiecutter/</link>
      <pubDate>Mon, 10 Apr 2023 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2023-04-10-cookie/cookiecutter/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;Recently, I started to put some scripts together and run them against a Kaggle dataset. I decided to train my skills on an unseen dataset. Training keeps me sharp, and I need it to complement my skill set. For the last 2,5 years, I struggled in a small team with NLP problems, where I worked mostly on engineering tasks. My understanding in this area is not where I wanted to be. And on top, I follow this natural human process called forgetting things. For example, I definitely can&amp;rsquo;t write all relevant stochiometric formulas of the chemo-lithotrophic denitrification by memory. This was very important during my Ph.D. When did I start to forget relevant information? It&amp;rsquo;s funny to think back.  ü§∑&lt;/p&gt;</description>
    </item>
    <item>
      <title>Stratified multi-label split</title>
      <link>http://localhost:1313/posts/2023-04-09-stratified/multi_label_split/</link>
      <pubDate>Sat, 08 Apr 2023 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2023-04-09-stratified/multi_label_split/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;This post is a short overview of a stratified multi-label train-test split. Please look at the &lt;a href=&#34;https://colab.research.google.com/drive/1SdJKRef4sZYowuddGIOzN8PVGH9PngyF?usp=sharing&#34;&gt;colab implementation&lt;/a&gt; for a step through guide.&lt;/p&gt;
&lt;p&gt;Sometimes you step into work problems, which justify a small post. I already saw colleagues struggling to balance the train-test split for multi-label classification.
In classification problems, we have often a dataset with an imbalanced number of classes. In general, it is desired to keep the proportions of each label for the train and test sets as observed as in the original dataset. This stratified train-test split works well with single-label classification problems. For multi-label classification it is unclear how stratified sampling should be performed. Therefor &lt;a href=&#34;http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf&#34;&gt;Sechidis et al. 2011&lt;/a&gt; and &lt;a href=&#34;http://proceedings.mlr.press/v74/szyma%C5%84ski17a/szyma%C5%84ski17a.pdf&#34;&gt;Szymanski and Kajdanowicz 2017&lt;/a&gt; developed an algorithm to provide balanced datasets for multi-label classification. The documentation of their algorithm can be found &lt;a href=&#34;http://scikit.ml/stratification.html&#34;&gt;in the scikit-multilearn package&lt;/a&gt; and &lt;a href=&#34;https://github.com/scikit-multilearn/scikit-multilearn/blob/master/skmultilearn/model_selection/iterative_stratification.py#L175&#34;&gt;on github&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding scaled-dot product attention and multi-head attention</title>
      <link>http://localhost:1313/posts/2023-03-19-attention/attention_layer/</link>
      <pubDate>Sat, 18 Mar 2023 10:30:58 +0100</pubDate>
      <guid>http://localhost:1313/posts/2023-03-19-attention/attention_layer/</guid>
      <description>&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;This post is a summary of my implementation of the scaled-dot product attention and multi-head attention. Please have a look at the &lt;a href=&#34;https://colab.research.google.com/drive/1sE0B4NOAu9kqUTpuHIp1EYlD5JCAw4Gq?usp=sharing&#34;&gt;colab implementation&lt;/a&gt; for a step through guide.&lt;/p&gt;
&lt;p&gt;Even though this post is five years too late, the best way of reviving knowledge is to write about it. Transformers are in transforming the world via ChatGPT, Bart, or LLama. The core of the transformer architecture is the self-attention layer. There are many attention mechanisms (listed in this &lt;a href=&#34;https://lilianweng.github.io/posts/2018-06-24-attention/&#34;&gt;great post by Lilian Weng&lt;/a&gt;), but the scaled-dot product attention layer is used in general (&lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34;&gt;Vaswani et al. 2017&lt;/a&gt;). For a visual explanation of the transformer, look at the great post from &lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;Jay Alammar&lt;/a&gt;. Please check &lt;a href=&#34;https://www.youtube.com/watch?v=kCc8FmEb1nY&#34;&gt;Andrej Karpathy&amp;rsquo;s video&lt;/a&gt; for the full implementation of a transformer from scratch.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
