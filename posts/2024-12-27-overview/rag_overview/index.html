<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Overview of RAG (Retrieval-Augmented Generation) systems | About me</title>
<meta name="keywords" content="nlp, machine learning, system design">
<meta name="description" content="Hi,
It’s been a while since my last post, mostly because of my own laziness. Over the past year, I’ve been working on several projects, one of which is a small RAG (Retrieval-Augmented Generation) system. I implemented it to combine external knowledge (in this case internal safety documents) with a large language model (LLM). This approach allows the use of data that the LLM wasn&rsquo;t trained on and also helps reduce hallucinations.">
<meta name="author" content="">
<link rel="canonical" href="https://steffenhaeussler.github.io/posts/2024-12-27-overview/rag_overview/">
<link crossorigin="anonymous" href="https://steffenhaeussler.github.io/assets/css/stylesheet.c32c3fa257f14f25741bf9b4bf26c500e50fe88bb7915cdb5f59134fcd019ee5.css" integrity="sha256-wyw/olfxTyV0G/m0vybFAOUP6Iu3kVzbX1kTT80BnuU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://steffenhaeussler.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://steffenhaeussler.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://steffenhaeussler.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://steffenhaeussler.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://steffenhaeussler.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://steffenhaeussler.github.io/posts/2024-12-27-overview/rag_overview/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta property="og:url" content="https://steffenhaeussler.github.io/posts/2024-12-27-overview/rag_overview/">
  <meta property="og:site_name" content="About me">
  <meta property="og:title" content="Overview of RAG (Retrieval-Augmented Generation) systems">
  <meta property="og:description" content="Hi,
It’s been a while since my last post, mostly because of my own laziness. Over the past year, I’ve been working on several projects, one of which is a small RAG (Retrieval-Augmented Generation) system. I implemented it to combine external knowledge (in this case internal safety documents) with a large language model (LLM). This approach allows the use of data that the LLM wasn’t trained on and also helps reduce hallucinations.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-27T10:30:58+01:00">
    <meta property="article:modified_time" content="2024-12-27T10:30:58+01:00">
    <meta property="article:tag" content="Nlp">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="System Design">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Overview of RAG (Retrieval-Augmented Generation) systems">
<meta name="twitter:description" content="Hi,
It’s been a while since my last post, mostly because of my own laziness. Over the past year, I’ve been working on several projects, one of which is a small RAG (Retrieval-Augmented Generation) system. I implemented it to combine external knowledge (in this case internal safety documents) with a large language model (LLM). This approach allows the use of data that the LLM wasn&rsquo;t trained on and also helps reduce hallucinations.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://steffenhaeussler.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Overview of RAG (Retrieval-Augmented Generation) systems",
      "item": "https://steffenhaeussler.github.io/posts/2024-12-27-overview/rag_overview/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Overview of RAG (Retrieval-Augmented Generation) systems",
  "name": "Overview of RAG (Retrieval-Augmented Generation) systems",
  "description": "Hi,\nIt’s been a while since my last post, mostly because of my own laziness. Over the past year, I’ve been working on several projects, one of which is a small RAG (Retrieval-Augmented Generation) system. I implemented it to combine external knowledge (in this case internal safety documents) with a large language model (LLM). This approach allows the use of data that the LLM wasn\u0026rsquo;t trained on and also helps reduce hallucinations.\n",
  "keywords": [
    "nlp", "machine learning", "system design"
  ],
  "articleBody": "Hi,\nIt’s been a while since my last post, mostly because of my own laziness. Over the past year, I’ve been working on several projects, one of which is a small RAG (Retrieval-Augmented Generation) system. I implemented it to combine external knowledge (in this case internal safety documents) with a large language model (LLM). This approach allows the use of data that the LLM wasn’t trained on and also helps reduce hallucinations.\nA RAG system consists on several components like embeddings, vector storage, ranking and answer generation. This post gives just a brief overview, but I plan a deep-dive into each part. Please have a look at following papers:\nRetrieval-Augmented Generation for LLM: A Survey by Gao et al. 2024 Corrective retrieval augmented generation by Yan et al. 2024 RAG Cheat sheet Fig.1: General overview of a RAG system\nEmbeddings The embeddings component converts data into numerical representations or vectors, capturing its semantic meaning. These embeddings enable the system to identify similar data points. Embeddings can be created from multiple data types (text, image, audio, video) by pre-trained models such as BERT, SentenceTransformer, ResNet, or Whisper. These models can also be fine-tuned on your specific data or even trained from scratch.\nThe quality of embeddings impacts retrieval accurancy and overall system performance. Additionally, the granularity of the embedded data plays a crucial role: coarse-grained data contain more information, but can distract the retriever and LLM, while fine-grained data can lack essential knowledge. Typically data is split into chunks using techniques such as fixed-length windows, sliding windows, or others. Semi-structured text is more challenging, as tables and images require different methods to create embeddings. To enhance or accelerate retrieval, metadata can be stored alongside the chunks.\nRetrieval and Vector Database The retrieval process identifies the most relevant documents for a user query by calculating similarity scores, often using cosine similarity, between the query embedding and stored embeddings. Vector databases are optimized for handling high-dimensional embeddings at scale via the use of special indexing techniques such as approximate nearest neighbor (ANN).\nDuring retrieval, the vector database returns the top N most similar results. This component prioritize low latency, often at the expense of accuracy, which is why an additional reranking step is commonly recommended to refine the results. Popular vector databases are e.g. PineCone, Qdrant, ElasticSearch or FAISS.\nRanking The ranking component orders the N candidates retrieved earlier based on their relevance to the user query. This is typically achieved using a model that applies more complex scoring criteria. While reranking improves the quality of results, not all systems require it, as it adds more complexity to the system.\nReranking models often need to be fine-tuned for the specific task, but suitable training data may not always be available. In such cases, additional steps may be required to generate training data, such as semi-supervised learning, active learning, or data generation using LLMs. Typical models for reranking are cross-encoders like bge-reranker or ms-marco. Alternatively, prompting a large language model for ranking is another option.\nGeneration The LLM generates a response by combining the user query and the selected documents into a single prompt. The quality and relevance of the response depends on the instructions provided in the prompt. For chatbots, conversational history can also be included to maintain context.\nTypically, only one retrieval step is performed. However, for complex problems or multi-step reasoning, multiple LLM requests may be required. This process, often referred to as augmentation, involves the LLM acting as a judge or orchestrator. Detailed discussion of augmentation is beyond the scope of this post.\nEvaluation RAG evaluation involves two distinct targets: retrieval and generation. For retrieval, context relevance and noise robustness are key factors in assessing quality, while for generation, key factors like answer faithfulness, answer relevance, negative rejection, information integration, and counterfactual robustness are important.\nRetrieval aspects:\nContext relevance: Ensures that retrieved information is precise and directly related to the query. Noise robustness: Measures the system’s ability to ignore unrelated or irrelevant documents. Generation aspects:\nAnswer faithfulness: Ensures that the generated answers remain true to the retrieved context. Answer relevance: Assesses whether the generated response directly addresses the user query. Negative rejection: Evaluates the system’s ability to refrain from answering when insufficient reliable information is available. Information integration: Measures the system’s capability to synthesize knowledge from multiple sources. Counterfactual robustness: Ensures the identification and dismissal of incorrect or misleading information. The primary goal is to ensure relevance and consistency while avoiding contradictions. RAG evaluation is more complex than many other NLP tasks. Framework like RAGES provide tools for evaluation. Alternative, Gao et al. 2024 combines specific metrics (e.g. F1-score, Precision, Recall and so on) to each of those listed aspects.\nThis is a brief overview of the components of a RAG system.\nThank you for your attention.\n",
  "wordCount" : "797",
  "inLanguage": "en",
  "datePublished": "2024-12-27T10:30:58+01:00",
  "dateModified": "2024-12-27T10:30:58+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://steffenhaeussler.github.io/posts/2024-12-27-overview/rag_overview/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "About me",
    "logo": {
      "@type": "ImageObject",
      "url": "https://steffenhaeussler.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://steffenhaeussler.github.io/" accesskey="h" title="About me (Alt + H)">About me</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://steffenhaeussler.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://steffenhaeussler.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://steffenhaeussler.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://steffenhaeussler.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://steffenhaeussler.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Overview of RAG (Retrieval-Augmented Generation) systems
    </h1>
    <div class="post-meta"><span title='2024-12-27 10:30:58 +0100 CET'>December 27, 2024</span>&nbsp;·&nbsp;4 min

</div>
  </header> 
  <div class="post-content"><p>Hi,</p>
<p>It’s been a while since my last post, mostly because of my own laziness. Over the past year, I’ve been working on several projects, one of which is a small RAG (Retrieval-Augmented Generation) system. I implemented it to combine external knowledge (in this case internal safety documents) with a large language model (LLM). This approach allows the use of data that the LLM wasn&rsquo;t trained on and also helps reduce hallucinations.</p>
<p>A RAG system consists on several components like embeddings, vector storage, ranking and answer generation. This post gives just a brief overview, but I plan a deep-dive into each part. Please have a look at following papers:</p>
<ul>
<li>Retrieval-Augmented Generation for LLM: A Survey by <a href="https://arxiv.org/pdf/2312.10997">Gao et al. 2024</a></li>
<li>Corrective retrieval augmented generation by <a href="https://arxiv.org/pdf/2401.15884">Yan et al. 2024</a></li>
<li><a href="https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b">RAG Cheat sheet</a></li>
</ul>
<p><img alt="RAG system" loading="lazy" src="https://steffenhaeussler.github.io/posts/2024-12-27-overview/rag_system_dark.png#center" title="Overview RAG system">
Fig.1: General overview of a RAG system</p>
<h2 id="embeddings">Embeddings<a hidden class="anchor" aria-hidden="true" href="#embeddings">#</a></h2>
<p>The embeddings component converts data into numerical representations or vectors, capturing its semantic meaning. These embeddings enable the system to identify similar data points. Embeddings can be created from multiple data types (text, image, audio, video) by pre-trained models such as <a href="https://huggingface.co/docs/transformers/main/en/model_doc/bert">BERT</a>, <a href="https://huggingface.co/sentence-transformers">SentenceTransformer</a>, <a href="https://huggingface.co/docs/transformers/en/model_doc/resnet">ResNet</a>, or <a href="https://huggingface.co/openai/whisper-large-v3-turbo">Whisper</a>. These models can also be fine-tuned on your specific data or even trained from scratch.</p>
<p>The quality of embeddings impacts retrieval accurancy and overall system performance. Additionally, the granularity of the embedded data plays a crucial role: coarse-grained data contain more information, but can distract the retriever and LLM, while fine-grained data can lack essential knowledge. Typically data is split into chunks using techniques such as fixed-length windows, sliding windows, or others. Semi-structured text is more challenging, as tables and images require different methods to create embeddings. To enhance or accelerate retrieval, metadata can be stored alongside the chunks.</p>
<h2 id="retrieval-and-vector-database">Retrieval and Vector Database<a hidden class="anchor" aria-hidden="true" href="#retrieval-and-vector-database">#</a></h2>
<p>The retrieval process identifies the most relevant documents for a user query by calculating similarity scores, often using cosine similarity, between the query embedding and stored embeddings. Vector databases are optimized for handling high-dimensional embeddings at scale via the use of special indexing techniques such as approximate nearest neighbor (ANN).</p>
<p>During retrieval, the vector database returns the top N most similar results. This component prioritize low latency, often at the expense of accuracy, which is why an additional reranking step is commonly recommended to refine the results. Popular vector databases are e.g. <a href="https://github.com/pinecone-io">PineCone</a>, <a href="https://github.com/qdrant/qdrant">Qdrant</a>, <a href="https://github.com/elastic/elasticsearch">ElasticSearch</a> or <a href="https://github.com/facebookresearch/faiss">FAISS</a>.</p>
<h2 id="ranking">Ranking<a hidden class="anchor" aria-hidden="true" href="#ranking">#</a></h2>
<p>The ranking component orders the N candidates retrieved earlier based on their relevance to the user query. This is typically achieved using a model that applies more complex scoring criteria. While reranking improves the quality of results, not all systems require it, as it adds more complexity to the system.</p>
<p>Reranking models often need to be fine-tuned for the specific task, but suitable training data may not always be available. In such cases, additional steps may be required to generate training data, such as semi-supervised learning, active learning, or data generation using LLMs. Typical models for reranking are cross-encoders like <a href="https://huggingface.co/BAAI/bge-reranker-large">bge-reranker</a> or <a href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2">ms-marco</a>. Alternatively, prompting a large language model for ranking is another option.</p>
<h2 id="generation">Generation<a hidden class="anchor" aria-hidden="true" href="#generation">#</a></h2>
<p>The LLM generates a response by combining the user query and the selected documents into a single prompt. The quality and relevance of the response depends on the instructions provided in the prompt. For chatbots, conversational history can also be included to maintain context.</p>
<p>Typically, only one retrieval step is performed. However, for complex problems or multi-step reasoning, multiple LLM requests may be required. This process, often referred to as augmentation, involves the LLM acting as a judge or orchestrator. Detailed discussion of augmentation is beyond the scope of this post.</p>
<h2 id="evaluation">Evaluation<a hidden class="anchor" aria-hidden="true" href="#evaluation">#</a></h2>
<p>RAG evaluation involves two distinct targets: retrieval and generation. For retrieval, context relevance and noise robustness are key factors in assessing quality, while for generation, key factors like answer faithfulness, answer relevance, negative rejection, information integration, and counterfactual robustness are important.</p>
<p>Retrieval aspects:</p>
<ul>
<li>Context relevance: Ensures that retrieved information is precise and directly related to the query.</li>
<li>Noise robustness: Measures the system&rsquo;s ability to ignore unrelated or irrelevant documents.</li>
</ul>
<p>Generation aspects:</p>
<ul>
<li>Answer faithfulness: Ensures that the generated answers remain true to the retrieved context.</li>
<li>Answer relevance: Assesses whether the generated response directly addresses the user query.</li>
<li>Negative rejection: Evaluates the system&rsquo;s ability to refrain from answering when insufficient reliable information is available.</li>
<li>Information integration: Measures the system’s capability to synthesize knowledge from multiple sources.</li>
<li>Counterfactual robustness: Ensures the identification and dismissal of incorrect or misleading information.</li>
</ul>
<p>The primary goal is to ensure relevance and consistency while avoiding contradictions. RAG evaluation is more complex than many other NLP tasks. Framework like <a href="https://github.com/explodinggradients/ragas">RAGES</a> provide tools for evaluation. Alternative, <a href="https://arxiv.org/pdf/2312.10997#page=14">Gao et al. 2024</a> combines specific metrics (e.g. F1-score, Precision, Recall and so on) to each of those listed aspects.</p>
<p>This is a brief overview of the components of a RAG system.</p>
<p>Thank you for your attention.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://steffenhaeussler.github.io/tags/nlp/">Nlp</a></li>
      <li><a href="https://steffenhaeussler.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://steffenhaeussler.github.io/tags/system-design/">System Design</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://steffenhaeussler.github.io/">About me</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
