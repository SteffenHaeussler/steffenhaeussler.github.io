<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Deep Learning model explainability | About me</title>
<meta name="keywords" content="data science, machine learning, deep learning, explainability">
<meta name="description" content="Hi,
In my first post, I looked into the explainability of classical machine learning models. As a next step, I&rsquo;m interested in the explainability of neural networks.  Model explainability is easy for simple models (linear regression, decision trees), and some tools exist for more complex algorithms (ensemble trees). Therefore, I highly recommend the book Interpretable Machine Learning by Christoph Molnar for a deeper theoretical understanding. All different approaches for model explanability are shown with a PyTorch model in this kaggle notebook.">
<meta name="author" content="">
<link rel="canonical" href="https://steffenhaeussler.github.io/posts/2023-12-08-dl-explainability/dl_model_explainability/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.c32c3fa257f14f25741bf9b4bf26c500e50fe88bb7915cdb5f59134fcd019ee5.css" integrity="sha256-wyw/olfxTyV0G/m0vybFAOUP6Iu3kVzbX1kTT80BnuU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://steffenhaeussler.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://steffenhaeussler.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://steffenhaeussler.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://steffenhaeussler.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://steffenhaeussler.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://steffenhaeussler.github.io/posts/2023-12-08-dl-explainability/dl_model_explainability/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<meta property="og:url" content="https://steffenhaeussler.github.io/posts/2023-12-08-dl-explainability/dl_model_explainability/">
  <meta property="og:site_name" content="About me">
  <meta property="og:title" content="Deep Learning model explainability">
  <meta property="og:description" content="Hi,
In my first post, I looked into the explainability of classical machine learning models. As a next step, I’m interested in the explainability of neural networks. Model explainability is easy for simple models (linear regression, decision trees), and some tools exist for more complex algorithms (ensemble trees). Therefore, I highly recommend the book Interpretable Machine Learning by Christoph Molnar for a deeper theoretical understanding. All different approaches for model explanability are shown with a PyTorch model in this kaggle notebook.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-12-08T10:30:58+01:00">
    <meta property="article:modified_time" content="2023-12-08T10:30:58+01:00">
    <meta property="article:tag" content="Data Science">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Deep Learning">
    <meta property="article:tag" content="Explainability">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Learning model explainability">
<meta name="twitter:description" content="Hi,
In my first post, I looked into the explainability of classical machine learning models. As a next step, I&rsquo;m interested in the explainability of neural networks.  Model explainability is easy for simple models (linear regression, decision trees), and some tools exist for more complex algorithms (ensemble trees). Therefore, I highly recommend the book Interpretable Machine Learning by Christoph Molnar for a deeper theoretical understanding. All different approaches for model explanability are shown with a PyTorch model in this kaggle notebook.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://steffenhaeussler.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Deep Learning model explainability",
      "item": "https://steffenhaeussler.github.io/posts/2023-12-08-dl-explainability/dl_model_explainability/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Deep Learning model explainability",
  "name": "Deep Learning model explainability",
  "description": "Hi,\nIn my first post, I looked into the explainability of classical machine learning models. As a next step, I\u0026rsquo;m interested in the explainability of neural networks. Model explainability is easy for simple models (linear regression, decision trees), and some tools exist for more complex algorithms (ensemble trees). Therefore, I highly recommend the book Interpretable Machine Learning by Christoph Molnar for a deeper theoretical understanding. All different approaches for model explanability are shown with a PyTorch model in this kaggle notebook.\n",
  "keywords": [
    "data science", "machine learning", "deep learning", "explainability"
  ],
  "articleBody": "Hi,\nIn my first post, I looked into the explainability of classical machine learning models. As a next step, I’m interested in the explainability of neural networks. Model explainability is easy for simple models (linear regression, decision trees), and some tools exist for more complex algorithms (ensemble trees). Therefore, I highly recommend the book Interpretable Machine Learning by Christoph Molnar for a deeper theoretical understanding. All different approaches for model explanability are shown with a PyTorch model in this kaggle notebook.\nComplex models produce helpful predictions but often behave as a black box. This means that an explanation of the model’s behavior doesn’t exist. However, stakeholders often request an explanation of the model prediction to check for causality and to gain trust in the model.\nI would say that Captum is the standard library for model interpretability with Pytorch. Captum also includes implementations to calculate SHAP values or LIME. This overview shows all implemented algorithms including application and complexity. Captum separates the algorithms into three groups (primary attribution, layer attribution, and neuron attribution). Primary attribution evaluates the effect of each feature on the model output. Layer attribution evaluates the effect of each neuron of a given layer on the model output. Neuron attribution evaluates the effect of an input feature on a specific particular neuron.\nPrimary Attribution Primary attribution evaluates the effect of each feature on the model output. The following algorithms are used in this example:\nIntegrated Gradients Integrated Gradients is an axiomatic model interpretability algorithm that assigns an importance score to each input feature by approximating the integral of gradients of the model’s output concerning the inputs along the path (straight line) from given baselines/references to inputs.\nAxiomatic Attribution for Deep Networks by Sundararajan et al. 2017 Implementation Gradient SHAP Gradient SHAP is a gradient method to compute SHAP values. Gradient SHAP adds Gaussian noise to each input sample multiple times, selects a random point along the path between baseline and input, and computes the gradient of outputs concerning those selected random points. The final SHAP values represent the expected value of gradients * (inputs - baselines). Gradient SHAP assumes that each feature is independent and the explanation model has a linear relationship between the inputs and the given baseline.\nA Unified Approach to Interpreting Model Predictions by Lundberg and Lee 2017 Implementation DeepLIFT DeepLIFT is a back-propagation-based approach that attributes a change to inputs based on the differences between the inputs and corresponding references (or baselines) for non-linear activations. DeepLIFT seeks to explain the difference in the output from reference in terms of the difference in inputs from reference. DeepLIFT uses the concept of multipliers to “blame” specific neurons for the difference in the output.\nLearning Important Features Through Propagating Activation Differences by Shrikumar et al. 2017 Towards Better Understanding of Gradient-Based Attribution Methods for Deep Neural Networks by Ancona et al. 2018 Implementation Feature Ablation Feature ablation is a perturbation-based approach to compute attribution, which involves replacing each input feature with a given baseline/reference value, and computing the difference in the output. Input features can also be grouped and ablated together rather than individually.\nImplementation Noise Tunnel Noise Tunnel is a method that can be used on top of any of the attribution methods. Noise tunnel computes attribution multiple times, adding Gaussian noise to the input each time, and combines the calculated attributions based on the chosen type. The supported types for noise tunnels are:\nSmoothgrad: The mean of the sampled attributions is returned. This approximates smoothing the given attribution method with a Gaussian Kernel. Smoothgrad Squared: The mean of the squared sample attributions is returned. Vargrad: The variance of the sample attributions is returned.\nSmoothGrad: removing noise by adding noise by Smilkov et al. 2017 Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values by Adebayo et al. 2018 Implementation Feature importance Feature importance describes how useful a feature is at predicting a target variable. It also takes into account all interactions with other features. Additionally, feature importance can also used for dimensionality reduction and feature selection. There are two ways of computing the feature importance. One way is via impurity importance (mean decrease of impurity), and the second way is via permutation importance (average decrease accuracy). Permutation importance is model agnostic and should be preferred. A general drawback is correlated features. In this case, both features will score a lower importance, where they will be important.\nLayer Attribution As the next step, let’s look at each neuron on a layer.\nLayer Conductance Conductance combines the neuron activation with the partial derivatives of both the neuron concerning the input and the output concerning the neuron to build a more complete picture of neuron importance.\nConductance builds on Integrated Gradients (IG) by looking at the flow of IG attribution, which occurs through the hidden neuron.\nHow Important Is a Neuron? by Dhamdhere et al. 2018 Computationally Efficient Measures of Internal Neuron Importance by Shrikumar et al. 2018 Implementation Layer Activation Layer Activation is a simple approach for computing layer attribution, returning the activation of each neuron in the identified layer.\nImplementation Layer Integrated Gradients Layer Integrated Gradients is a variant of Integrated Gradients that assigns an importance score to layer inputs or outputs, depending on whether we attribute to the former or the latter. Integrated Gradients is an axiomatic model interpretability algorithm that attributes/assigns an importance score to each input feature by approximating the integral of gradients of the model’s output concerning the inputs along the path (straight line) from given baselines/references to inputs.\nImplementation Neuron Attribution We can also look at the distribution of each neuron’s attributions.\nNeuron Conductance Conductance combines the neuron activation with the partial derivatives of both the neuron concerning the input and the output concerning the neuron to build a more complete picture of neuron importance.\nConductance for a particular neuron builds on Integrated Gradients (IG) by looking at the flow of IG attribution from each input through a particular neuron.\nHow Important Is a Neuron? by Dhamdhere et al. 2018 Computationally Efficient Measures of Internal Neuron Importance by Shrikumar et al. 2018 Implementation Shapley values SHAP (Shapley Additive exPlanations) is a method to explain individual predictions. It is based on Shapley’s values and originates from cooperative game theory. It is a method for assigning payouts to players depending on their contribution to the total payout. Players cooperate in a coalition and receive a profit from this cooperation. Machine learning uses the difference of a single prediction towards the average prediction. The Shapley value is the average contribution of a feature value across all possible coalitions.​Keep in mind that the calculation of Shapley values can be computationally expensive. The difference between the SHAP value and the average prediction is fairly distributed among the feature values. This allows a contrastive explanation down to one single data point. The Shapley value returns a simple value per feature without a prediction model. Shap cannot be used to make statements about changes in prediction for changes in the input. Another disadvantage is that the calculation needs the training data. And like other permutation-based methods, Shapley values suffer from correlated features.​You can find additional information here:\nA Unified Approach to Interpreting Model Predictions by Lundberg and Lee 2017 SHAP implementation With a customized prediction function, we can use the SHAP framework.\nLIME Local interpretable model-agnostic explanations (LIME) is a paper in which the authors propose a concrete implementation of interpretable models to explain individual predictions of machine learning models. LIME trains simple models to approximate the predictions of the underlying model. As a first step, a new dataset is generated consisting of perturbed samples with corresponding predictions of the model. On this new dataset, LIME trains an interpretable model (decision tree, linear regression), which is weighted by the proximity of the sampled instances to the instance of interest.\nOne problem is that the calculation of artificial datasets has its weaknesses. Multiple settings need to be tested before a final LIME model can be used. Another problem is that the interpretability of close points can vary greatly. This instability leads to a critical evaluation of any explanation.\nYou can find additional information here:\n“Why Should I Trust You?”: Explaining the Predictions of Any Classifier by Rebeiro et al. 2016 LIME implementation for tabular data Thank you for your attention.\n",
  "wordCount" : "1385",
  "inLanguage": "en",
  "datePublished": "2023-12-08T10:30:58+01:00",
  "dateModified": "2023-12-08T10:30:58+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://steffenhaeussler.github.io/posts/2023-12-08-dl-explainability/dl_model_explainability/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "About me",
    "logo": {
      "@type": "ImageObject",
      "url": "https://steffenhaeussler.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://steffenhaeussler.github.io/" accesskey="h" title="About me (Alt + H)">About me</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://steffenhaeussler.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://steffenhaeussler.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://steffenhaeussler.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://steffenhaeussler.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://steffenhaeussler.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Deep Learning model explainability
    </h1>
    <div class="post-meta"><span title='2023-12-08 10:30:58 +0100 CET'>December 8, 2023</span>&nbsp;·&nbsp;7 min

</div>
  </header> 
  <div class="post-content"><p>Hi,</p>
<p><a href="https://steffenhaeussler.github.io/posts/model_explainability/">In my first post</a>, I looked into the explainability of classical machine learning models. As a next step, I&rsquo;m interested in the explainability of neural networks.  Model explainability is easy for simple models (linear regression, decision trees), and some tools exist for more complex algorithms (ensemble trees). Therefore, I highly recommend the book <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning by Christoph Molnar</a> for a deeper theoretical understanding. All different approaches for model explanability are shown with a PyTorch model <a href="https://www.kaggle.com/code/steffenhaeussler/mohs-hardness-model-explainer-for-neuralnets/notebook">in this kaggle notebook</a>.</p>
<p>Complex models produce helpful predictions but often behave as a black box. This means that an explanation of the model&rsquo;s behavior doesn&rsquo;t exist. However, stakeholders often request an explanation of the model prediction to check for causality and to gain trust in the model.</p>
<p>I would say that <a href="https://captum.ai/">Captum</a> is the standard library for model interpretability with Pytorch. Captum also includes implementations to calculate SHAP values or LIME. <a href="https://captum.ai/docs/algorithms_comparison_matrix">This overview</a> shows all implemented algorithms including application and complexity. Captum separates the algorithms into three groups (primary attribution, layer attribution, and neuron attribution). Primary attribution evaluates the effect of each feature on the model output. Layer attribution evaluates the effect of each neuron of a given layer on the model output. Neuron attribution evaluates the effect of an input feature on a specific particular neuron.</p>
<h2 id="primary-attribution">Primary Attribution<a hidden class="anchor" aria-hidden="true" href="#primary-attribution">#</a></h2>
<p>Primary attribution evaluates the effect of each feature on the model output. The following algorithms are used in this example:</p>
<h3 id="integrated-gradients">Integrated Gradients<a hidden class="anchor" aria-hidden="true" href="#integrated-gradients">#</a></h3>
<p>Integrated Gradients is an axiomatic model interpretability algorithm that assigns an importance score to each input feature by approximating the integral of gradients of the model&rsquo;s output concerning the inputs along the path (straight line) from given baselines/references to inputs.</p>
<ul>
<li>Axiomatic Attribution for Deep Networks by <a href="https://arxiv.org/abs/1703.01365">Sundararajan et al. 2017</a></li>
<li><a href="https://github.com/pytorch/captum/blob/master/captum/attr/_core/integrated_gradients.py#L109">Implementation</a></li>
</ul>
<h3 id="gradient-shap">Gradient SHAP<a hidden class="anchor" aria-hidden="true" href="#gradient-shap">#</a></h3>
<p>Gradient SHAP is a gradient method to compute SHAP values. Gradient SHAP adds Gaussian noise to each input sample multiple times, selects a random point along the path between baseline and input, and computes the gradient of outputs concerning those selected random points. The final SHAP values represent the expected value of gradients * (inputs - baselines). Gradient SHAP assumes that each feature is independent and the explanation model has a linear relationship between the inputs and the given baseline.</p>
<ul>
<li>A Unified Approach to Interpreting Model Predictions by <a href="https://arxiv.org/abs/1705.07874">Lundberg and Lee 2017</a></li>
<li><a href="https://github.com/pytorch/captum/blob/master/captum/attr/_core/gradient_shap.py#L113">Implementation</a></li>
</ul>
<h3 id="deeplift">DeepLIFT<a hidden class="anchor" aria-hidden="true" href="#deeplift">#</a></h3>
<p>DeepLIFT is a back-propagation-based approach that attributes a change to inputs based on the differences between the inputs and corresponding references (or baselines) for non-linear activations. DeepLIFT seeks to explain the difference in the output from reference in terms of the difference in inputs from reference. DeepLIFT uses the concept of multipliers to &ldquo;blame&rdquo; specific neurons for the difference in the output.</p>
<ul>
<li>Learning Important Features Through Propagating Activation Differences by <a href="https://arxiv.org/abs/1704.02685">Shrikumar et al. 2017</a></li>
<li>Towards Better Understanding of Gradient-Based Attribution Methods for Deep Neural Networks by <a href="https://openreview.net/pdf?id=Sy21R9JAW">Ancona et al. 2018</a></li>
<li><a href="https://github.com/pytorch/captum/blob/master/captum/attr/_core/deep_lift.py#L143">Implementation</a></li>
</ul>
<h3 id="feature-ablation">Feature Ablation<a hidden class="anchor" aria-hidden="true" href="#feature-ablation">#</a></h3>
<p>Feature ablation is a perturbation-based approach to compute attribution, which involves replacing each input feature with a given baseline/reference value, and computing the difference in the output. Input features can also be grouped and ablated together rather than individually.</p>
<ul>
<li><a href="https://github.com/pytorch/captum/blob/master/captum/attr/_core/feature_ablation.py#L67">Implementation</a></li>
</ul>
<h3 id="noise-tunnel">Noise Tunnel<a hidden class="anchor" aria-hidden="true" href="#noise-tunnel">#</a></h3>
<p>Noise Tunnel is a method that can be used on top of any of the attribution methods. Noise tunnel computes attribution multiple times, adding Gaussian noise to the input each time, and combines the calculated attributions based on the chosen type. The supported types for noise tunnels are:</p>
<p>Smoothgrad: The mean of the sampled attributions is returned. This approximates smoothing the given attribution method with a Gaussian Kernel.
Smoothgrad Squared: The mean of the squared sample attributions is returned.
Vargrad: The variance of the sample attributions is returned.</p>
<ul>
<li>SmoothGrad: removing noise by adding noise by <a href="https://arxiv.org/abs/1706.03825">Smilkov et al. 2017</a></li>
<li>Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values by <a href="https://arxiv.org/abs/1810.03307">Adebayo et al. 2018</a></li>
<li><a href="https://github.com/pytorch/captum/blob/master/captum/attr/_core/noise_tunnel.py#L78">Implementation</a></li>
</ul>
<h3 id="feature-importance">Feature importance<a hidden class="anchor" aria-hidden="true" href="#feature-importance">#</a></h3>
<p>Feature importance describes how useful a feature is at predicting a target variable. It also takes into account all interactions with other features. Additionally, feature importance can also used for dimensionality reduction and feature selection. There are two ways of computing the feature importance. One way is via impurity importance (mean decrease of impurity), and the second way is via permutation importance (average decrease accuracy). Permutation importance is model agnostic and should be preferred. A general drawback is correlated features. In this case, both features will score a lower importance, where they will be important.</p>
<h2 id="layer-attribution">Layer Attribution<a hidden class="anchor" aria-hidden="true" href="#layer-attribution">#</a></h2>
<p>As the next step, let&rsquo;s look at each neuron on a layer.</p>
<h3 id="layer-conductance">Layer Conductance<a hidden class="anchor" aria-hidden="true" href="#layer-conductance">#</a></h3>
<p>Conductance combines the neuron activation with the partial derivatives of both the neuron concerning the input and the output concerning the neuron to build a more complete picture of neuron importance.</p>
<p>Conductance builds on Integrated Gradients (IG) by looking at the flow of IG attribution, which occurs through the hidden neuron.</p>
<ul>
<li>How Important Is a Neuron? by <a href="https://arxiv.org/abs/1805.12233">Dhamdhere et al. 2018</a></li>
<li>Computationally Efficient Measures of Internal Neuron Importance by <a href="https://arxiv.org/abs/1807.09946">Shrikumar et al. 2018</a></li>
<li><a href="https://github.com/pytorch/captum/blob/master/captum/attr/_core/layer/layer_conductance.py#L103">Implementation</a></li>
</ul>
<h3 id="layer-activation">Layer Activation<a hidden class="anchor" aria-hidden="true" href="#layer-activation">#</a></h3>
<p>Layer Activation is a simple approach for computing layer attribution, returning the activation of each neuron in the identified layer.</p>
<ul>
<li><a href="https://github.com/pytorch/captum/blob/master/captum/attr/_core/layer/layer_activation.py#L48">Implementation</a></li>
</ul>
<h3 id="layer-integrated-gradients">Layer Integrated Gradients<a hidden class="anchor" aria-hidden="true" href="#layer-integrated-gradients">#</a></h3>
<p>Layer Integrated Gradients is a variant of Integrated Gradients that assigns an importance score to layer inputs or outputs, depending on whether we attribute to the former or the latter. Integrated Gradients is an axiomatic model interpretability algorithm that attributes/assigns an importance score to each input feature by approximating the integral of gradients of the model&rsquo;s output concerning the inputs along the path (straight line) from given baselines/references to inputs.</p>
<ul>
<li><a href="https://github.com/pytorch/captum/blob/master/captum/attr/_core/layer/layer_integrated_gradients.py#L162">Implementation</a></li>
</ul>
<h2 id="neuron-attribution">Neuron Attribution<a hidden class="anchor" aria-hidden="true" href="#neuron-attribution">#</a></h2>
<p>We can also look at the distribution of each neuron&rsquo;s attributions.</p>
<h3 id="neuron-conductance">Neuron Conductance<a hidden class="anchor" aria-hidden="true" href="#neuron-conductance">#</a></h3>
<p>Conductance combines the neuron activation with the partial derivatives of both the neuron concerning the input and the output concerning the neuron to build a more complete picture of neuron importance.</p>
<p>Conductance for a particular neuron builds on Integrated Gradients (IG) by looking at the flow of IG attribution from each input through a particular neuron.</p>
<ul>
<li>How Important Is a Neuron? by <a href="https://arxiv.org/abs/1805.12233">Dhamdhere et al. 2018</a></li>
<li>Computationally Efficient Measures of Internal Neuron Importance by <a href="https://arxiv.org/abs/1807.09946">Shrikumar et al. 2018</a></li>
<li><a href="https://github.com/pytorch/captum/blob/master/captum/attr/_core/neuron/neuron_conductance.py#L91">Implementation</a></li>
</ul>
<h2 id="shapley-values">Shapley values<a hidden class="anchor" aria-hidden="true" href="#shapley-values">#</a></h2>
<p>SHAP (Shapley Additive exPlanations) is a method to explain individual predictions. It is based on Shapley&rsquo;s values and originates from cooperative game theory. It is a method for assigning payouts to players depending on their contribution to the total payout. Players cooperate in a coalition and receive a profit from this cooperation. Machine learning uses the difference of a single prediction towards the average prediction. The Shapley value is the average contribution of a feature value across all possible coalitions.​Keep in mind that the calculation of Shapley values can be computationally expensive. The difference between the SHAP value and the average prediction is fairly distributed among the feature values. This allows a contrastive explanation down to one single data point. The Shapley value returns a simple value per feature without a prediction model. Shap cannot be used to make statements about changes in prediction for changes in the input. Another disadvantage is that the calculation needs the training data. And like other permutation-based methods, Shapley values suffer from correlated features.​You can find additional information here:</p>
<ul>
<li><a href="https://arxiv.org/abs/1705.07874">A Unified Approach to Interpreting Model Predictions by Lundberg and Lee 2017</a></li>
<li><a href="https://github.com/shap/shap/blob/master/shap/explainers/_kernel.py#L38">SHAP implementation</a></li>
</ul>
<p>With a customized prediction function, we can use the SHAP framework.</p>
<h2 id="lime">LIME<a hidden class="anchor" aria-hidden="true" href="#lime">#</a></h2>
<p>Local interpretable model-agnostic explanations (LIME) is a paper in which the authors propose a concrete implementation of interpretable models to explain individual predictions of machine learning models. LIME trains simple models to approximate the predictions of the underlying model. As a first step, a new dataset is generated consisting of perturbed samples with corresponding predictions of the model. On this new dataset, LIME trains an interpretable model (decision tree, linear regression), which is weighted by the proximity of the sampled instances to the instance of interest.</p>
<p>One problem is that the calculation of artificial datasets has its weaknesses. Multiple settings need to be tested before a final LIME model can be used. Another problem is that the interpretability of close points can vary greatly. This instability leads to a critical evaluation of any explanation.</p>
<p>You can find additional information here:</p>
<ul>
<li>&ldquo;Why Should I Trust You?&rdquo;: Explaining the Predictions of Any Classifier by <a href="https://arxiv.org/abs/1602.04938">Rebeiro et al. 2016</a></li>
<li><a href="https://github.com/marcotcr/lime/blob/master/lime/lime_tabular.py#L117">LIME implementation for tabular data</a></li>
</ul>
<p>Thank you for your attention.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://steffenhaeussler.github.io/tags/data-science/">Data Science</a></li>
      <li><a href="https://steffenhaeussler.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://steffenhaeussler.github.io/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="https://steffenhaeussler.github.io/tags/explainability/">Explainability</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://steffenhaeussler.github.io/">About me</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
