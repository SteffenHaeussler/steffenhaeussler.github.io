<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Training an embedding model | About me</title>
<meta name="keywords" content="machine learning, NLP, contrastive learning">
<meta name="description" content="Hi,
Based on my previous post, I need to write a correction. I was wrong.
I previously suggested using embeddings directly from pre-trained models. That turns out to be a bad idea, because the training objectives are fundamentally different. As an alternative, I have now explored training an embedding model using Contrastive Learning.
Please look at my notebook for the full code and details.
The main reason you should not use raw embeddings from pre-trained models is the anisotropy problem. The paper How Contextual are Contextualized Word Representations? from Ethayarajh et al. 2019 shows that the embeddings are clustered in a narrow cone of the vector space, making them almost useless for differentiation. So, if you calculate the cosine similarity between two completely different sentences, the result will be around 0.80.">
<meta name="author" content="">
<link rel="canonical" href="https://steffenhaeussler.github.io/posts/2026-01-09-contrastive/contrastive/">
<link crossorigin="anonymous" href="https://steffenhaeussler.github.io/assets/css/stylesheet.a6264de45f1158af551cf2fe7595e8651c772fa1286f4c0483059a3e826a3049.css" integrity="sha256-piZN5F8RWK9VHPL&#43;dZXoZRx3L6Eob0wEgwWaPoJqMEk=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://steffenhaeussler.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://steffenhaeussler.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://steffenhaeussler.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://steffenhaeussler.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://steffenhaeussler.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://steffenhaeussler.github.io/posts/2026-01-09-contrastive/contrastive/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta property="og:url" content="https://steffenhaeussler.github.io/posts/2026-01-09-contrastive/contrastive/">
  <meta property="og:site_name" content="About me">
  <meta property="og:title" content="Training an embedding model">
  <meta property="og:description" content="Hi,
Based on my previous post, I need to write a correction. I was wrong.
I previously suggested using embeddings directly from pre-trained models. That turns out to be a bad idea, because the training objectives are fundamentally different. As an alternative, I have now explored training an embedding model using Contrastive Learning.
Please look at my notebook for the full code and details.
The main reason you should not use raw embeddings from pre-trained models is the anisotropy problem. The paper How Contextual are Contextualized Word Representations? from Ethayarajh et al. 2019 shows that the embeddings are clustered in a narrow cone of the vector space, making them almost useless for differentiation. So, if you calculate the cosine similarity between two completely different sentences, the result will be around 0.80.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2026-01-09T10:30:58+01:00">
    <meta property="article:modified_time" content="2026-01-09T10:30:58+01:00">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Contrastive Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training an embedding model">
<meta name="twitter:description" content="Hi,
Based on my previous post, I need to write a correction. I was wrong.
I previously suggested using embeddings directly from pre-trained models. That turns out to be a bad idea, because the training objectives are fundamentally different. As an alternative, I have now explored training an embedding model using Contrastive Learning.
Please look at my notebook for the full code and details.
The main reason you should not use raw embeddings from pre-trained models is the anisotropy problem. The paper How Contextual are Contextualized Word Representations? from Ethayarajh et al. 2019 shows that the embeddings are clustered in a narrow cone of the vector space, making them almost useless for differentiation. So, if you calculate the cosine similarity between two completely different sentences, the result will be around 0.80.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://steffenhaeussler.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Training an embedding model",
      "item": "https://steffenhaeussler.github.io/posts/2026-01-09-contrastive/contrastive/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Training an embedding model",
  "name": "Training an embedding model",
  "description": "Hi,\nBased on my previous post, I need to write a correction. I was wrong.\nI previously suggested using embeddings directly from pre-trained models. That turns out to be a bad idea, because the training objectives are fundamentally different. As an alternative, I have now explored training an embedding model using Contrastive Learning.\nPlease look at my notebook for the full code and details.\nThe main reason you should not use raw embeddings from pre-trained models is the anisotropy problem. The paper How Contextual are Contextualized Word Representations? from Ethayarajh et al. 2019 shows that the embeddings are clustered in a narrow cone of the vector space, making them almost useless for differentiation. So, if you calculate the cosine similarity between two completely different sentences, the result will be around 0.80.\n",
  "keywords": [
    "machine learning", "NLP", "contrastive learning"
  ],
  "articleBody": "Hi,\nBased on my previous post, I need to write a correction. I was wrong.\nI previously suggested using embeddings directly from pre-trained models. That turns out to be a bad idea, because the training objectives are fundamentally different. As an alternative, I have now explored training an embedding model using Contrastive Learning.\nPlease look at my notebook for the full code and details.\nThe main reason you should not use raw embeddings from pre-trained models is the anisotropy problem. The paper How Contextual are Contextualized Word Representations? from Ethayarajh et al. 2019 shows that the embeddings are clustered in a narrow cone of the vector space, making them almost useless for differentiation. So, if you calculate the cosine similarity between two completely different sentences, the result will be around 0.80.\nA method to fix this is described in the paper Supervised Contrastive Learning from Khosla et al. 2021. The idea is to use positive pairs (similar sentences) and negative pairs (dissimilar sentences) to train the model. Positve pairs are pulled close together and result in closer similarity, while negative pairs are pushed apart and result in a lower similarity. Supervised contrastive learning needs a labelled dataset to train the model. This takes a lot of time and effort to create.\nTo avoid the data labeling, I use SimCSE: Simple Contrastive Learning of Sentence Embeddings from Gao et al. 2022 to train the model. The idea is to create positive pairs on the same sentence only relying on dropout noise from the same model. By passing the same sentence through the encoder twice with standard dropout active, we introduce noise to create a valid positive pair for contrastive learning. Negative pairs are created by sampling random sentences from the dataset.\nFor SimCSE only two hyperparameters besides the model hyperparameters are critical:\nbatch size: this determines the number of negative samples available during training. Since SimCSE uses in-batch negatives, a batch size of 128 means the model sees 1 positive pair and 127 negative examples (N−1) for every step. Generally, a larger batch size makes the training task harder for the model. It forces the model to distinguish the correct sentence from a larger crowd of distractors, leading to more robust embeddings.\ntemperature: this parameter scales the cosine similarity scores before they are passed into the softmax function. It essentially controls the model’s sensitivity. A lower temperature (like the 0.05 used in the original SimCSE paper) sharpens the distribution, making the model very discriminative but potentially unstable. A higher temperature smooths the distribution out.\nHere is my implementation of SimCSE using the huggingface trainer class:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class SimCSETrainer(Trainer): def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None): outputs_a = model( input_ids=inputs[\"input_ids\"], # torch.Size([batch_size, 128]) attention_mask=inputs[\"attention_mask\"] # torch.Size([batch_size, 128]) ) # torch.Size([batch_size, 128, 768]) emb_a = outputs_a.last_hidden_state[:, 0, :] # [CLS] token torch.Size([batch_size, 768]) # Forward Pass 2 - Re-run same inputs for different Dropout mask outputs_b = model( input_ids=inputs[\"input_ids\"], # torch.Size([batch_size, 128]) attention_mask=inputs[\"attention_mask\"] # torch.Size([batch_size, 128]) ) # torch.Size([batch_size, 128, 768]) emb_b = outputs_b.last_hidden_state[:, 0, :] # [CLS] token torch.Size([batch_size, 768]) # Scales vector to L2 norm z_a = F.normalize(emb_a, p=2, dim=1) z_b = F.normalize(emb_b, p=2, dim=1) # Cosine Similarity Matrix sim_matrix = torch.matmul(z_a, z_b.T) / temperature # Labels are the diagonal (0, 1, 2...) batch_size = z_a.size(0) labels = torch.arange(batch_size).long().to(z_a.device) loss = F.cross_entropy(sim_matrix, labels) return (loss, outputs_a) if return_outputs else loss As you can see, the implementation is surprisingly simple.\nPlease have a look at the training results and the evaluation of the embeddings in the kaggle notebook. While there is still room for improvement, the embeddings are significantly better than the base model.\nOverall, I hope this post helps you start training your own embedding models. This skill is incredibly valuable for improving RAG (Retrieval-Augmented Generation) systems, especially when fine-tuning on your own specific datasets.\nThank you for your attention.\n",
  "wordCount" : "669",
  "inLanguage": "en",
  "datePublished": "2026-01-09T10:30:58+01:00",
  "dateModified": "2026-01-09T10:30:58+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://steffenhaeussler.github.io/posts/2026-01-09-contrastive/contrastive/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "About me",
    "logo": {
      "@type": "ImageObject",
      "url": "https://steffenhaeussler.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://steffenhaeussler.github.io/" accesskey="h" title="About me (Alt + H)">About me</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://steffenhaeussler.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://steffenhaeussler.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://steffenhaeussler.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://steffenhaeussler.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://steffenhaeussler.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Training an embedding model
    </h1>
    <div class="post-meta"><span title='2026-01-09 10:30:58 +0100 CET'>January 9, 2026</span>&nbsp;·&nbsp;4 min

</div>
  </header> 
  <div class="post-content"><p>Hi,</p>
<p>Based on my <a href="https://steffenhaeussler.github.io/posts/2025-01-02-embeddings/embeddings/">previous post</a>, I need to write a correction. I was wrong.</p>
<p>I previously suggested using embeddings directly from pre-trained models. That turns out to be a bad idea, because the training objectives are fundamentally different. As an alternative, I have now explored training an embedding model using Contrastive Learning.</p>
<p>Please look at my <a href="https://www.kaggle.com/code/steffenhaeussler/contrastive-learning">notebook</a> for the full code and details.</p>
<p>The main reason you should not use raw embeddings from pre-trained models is the anisotropy problem. The paper <a href="https://arxiv.org/pdf/1909.00512">How Contextual are Contextualized Word Representations? from Ethayarajh et al. 2019</a> shows that the embeddings are clustered in a narrow cone of the vector space, making them almost useless for differentiation. So, if you calculate the cosine similarity between two completely different sentences, the result will be around 0.80.</p>
<p>A method to fix this is described in the paper <a href="https://arxiv.org/pdf/2004.11362">Supervised Contrastive Learning from Khosla et al. 2021</a>. The idea is to use positive pairs (similar sentences) and negative pairs (dissimilar sentences) to train the model. Positve pairs are pulled close together and result in closer similarity, while negative pairs are pushed apart and result in a lower similarity. Supervised contrastive learning needs a labelled dataset to train the model. This takes a lot of time and effort to create.</p>
<p>To avoid the data labeling, I use <a href="https://arxiv.org/pdf/2104.08821">SimCSE: Simple Contrastive Learning of Sentence Embeddings from Gao et al. 2022</a> to train the model. The idea is to create positive pairs on the same sentence only relying on dropout noise from the same model. By passing the same sentence through the encoder twice with standard dropout active, we introduce noise to create a valid positive pair for contrastive learning. Negative pairs are created by sampling random sentences from the dataset.</p>
<p>For SimCSE only two hyperparameters besides the model hyperparameters are critical:</p>
<ul>
<li>
<p>batch size: this determines the number of negative samples available during training. Since SimCSE uses in-batch negatives, a batch size of 128 means the model sees 1 positive pair and 127 negative examples (N−1) for every step. Generally, a larger batch size makes the training task harder for the model. It forces the model to distinguish the correct sentence from a larger crowd of distractors, leading to more robust embeddings.</p>
</li>
<li>
<p>temperature: this parameter scales the cosine similarity scores before they are passed into the softmax function. It essentially controls the model&rsquo;s sensitivity. A lower temperature (like the 0.05 used in the original SimCSE paper) sharpens the distribution, making the model very discriminative but potentially unstable. A higher temperature smooths the distribution out.</p>
</li>
</ul>
<p>Here is my implementation of SimCSE using the huggingface trainer class:</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SimCSETrainer</span>(Trainer):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_loss</span>(self, model, inputs, return_outputs<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, num_items_in_batch<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        outputs_a <span style="color:#f92672">=</span> model(
</span></span><span style="display:flex;"><span>            input_ids<span style="color:#f92672">=</span>inputs[<span style="color:#e6db74">&#34;input_ids&#34;</span>], <span style="color:#75715e"># torch.Size([batch_size, 128])</span>
</span></span><span style="display:flex;"><span>            attention_mask<span style="color:#f92672">=</span>inputs[<span style="color:#e6db74">&#34;attention_mask&#34;</span>] <span style="color:#75715e"># torch.Size([batch_size, 128])</span>
</span></span><span style="display:flex;"><span>        ) <span style="color:#75715e"># torch.Size([batch_size, 128, 768])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        emb_a <span style="color:#f92672">=</span> outputs_a<span style="color:#f92672">.</span>last_hidden_state[:, <span style="color:#ae81ff">0</span>, :] <span style="color:#75715e"># [CLS] token torch.Size([batch_size, 768])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Forward Pass 2 - Re-run same inputs for different Dropout mask</span>
</span></span><span style="display:flex;"><span>        outputs_b <span style="color:#f92672">=</span> model(
</span></span><span style="display:flex;"><span>            input_ids<span style="color:#f92672">=</span>inputs[<span style="color:#e6db74">&#34;input_ids&#34;</span>], <span style="color:#75715e"># torch.Size([batch_size, 128])</span>
</span></span><span style="display:flex;"><span>            attention_mask<span style="color:#f92672">=</span>inputs[<span style="color:#e6db74">&#34;attention_mask&#34;</span>] <span style="color:#75715e"># torch.Size([batch_size, 128])</span>
</span></span><span style="display:flex;"><span>        ) <span style="color:#75715e"># torch.Size([batch_size, 128, 768])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        emb_b <span style="color:#f92672">=</span> outputs_b<span style="color:#f92672">.</span>last_hidden_state[:, <span style="color:#ae81ff">0</span>, :] <span style="color:#75715e"># [CLS] token torch.Size([batch_size, 768])</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Scales vector to L2 norm</span>
</span></span><span style="display:flex;"><span>        z_a <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(emb_a, p<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        z_b <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(emb_b, p<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Cosine Similarity Matrix</span>
</span></span><span style="display:flex;"><span>        sim_matrix <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(z_a, z_b<span style="color:#f92672">.</span>T) <span style="color:#f92672">/</span> temperature
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Labels are the diagonal (0, 1, 2...)</span>
</span></span><span style="display:flex;"><span>        batch_size <span style="color:#f92672">=</span> z_a<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        labels <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(batch_size)<span style="color:#f92672">.</span>long()<span style="color:#f92672">.</span>to(z_a<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>cross_entropy(sim_matrix, labels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> (loss, outputs_a) <span style="color:#66d9ef">if</span> return_outputs <span style="color:#66d9ef">else</span> loss
</span></span></code></pre></td></tr></table>
</div>
</div><p>As you can see, the implementation is surprisingly simple.</p>
<p>Please have a look at the training results and the evaluation of the embeddings in the <a href="https://www.kaggle.com/code/steffenhaeussler/contrastive-learning#Evaluation">kaggle notebook</a>. While there is still room for improvement, the embeddings are significantly better than the base model.</p>
<p>Overall, I hope this post helps you start training your own embedding models. This skill is incredibly valuable for improving RAG (Retrieval-Augmented Generation) systems, especially when fine-tuning on your own specific datasets.</p>
<p>Thank you for your attention.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://steffenhaeussler.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://steffenhaeussler.github.io/tags/nlp/">NLP</a></li>
      <li><a href="https://steffenhaeussler.github.io/tags/contrastive-learning/">Contrastive Learning</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://steffenhaeussler.github.io/">About me</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
