<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Understanding approximate nearest neighbor algorithm | About me</title>
<meta name="keywords" content="machine learning, algorithm, nlp">
<meta name="description" content="Hi,
This post is about the approximate nearest neighbor (ANN) algorithm. The code for this post is here, where I provide an example of using a framework and a python implementation. Most python implementation were written with the help of a LLM. I&rsquo;m amazed, how helpful they are for learning new things. I see them like a drunken professor, which with the right approach will be a very helpful tool.
As a next step in understanding RAGs, I want to have a closer look at approximate nearest neighbor algorithms. Basically, the purpose is to find the closest vector to a query vector in a database. Since I&rsquo;m also interested into the implementation, I follow mostly this amazing blog post. Vector search is the basic component of vector databases and their main purpose. ANN algorithms are looking for a close match instead of an exact match. This loss of accuracy allows an improvement of efficieny, which allows the search through much bigger datasets, high-dimensional data and real-time apllications.">
<meta name="author" content="">
<link rel="canonical" href="https://steffenhaeussler.github.io/posts/2025-04-19-ann/ann/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.c32c3fa257f14f25741bf9b4bf26c500e50fe88bb7915cdb5f59134fcd019ee5.css" integrity="sha256-wyw/olfxTyV0G/m0vybFAOUP6Iu3kVzbX1kTT80BnuU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://steffenhaeussler.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://steffenhaeussler.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://steffenhaeussler.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://steffenhaeussler.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://steffenhaeussler.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://steffenhaeussler.github.io/posts/2025-04-19-ann/ann/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta property="og:url" content="https://steffenhaeussler.github.io/posts/2025-04-19-ann/ann/">
  <meta property="og:site_name" content="About me">
  <meta property="og:title" content="Understanding approximate nearest neighbor algorithm">
  <meta property="og:description" content="Hi,
This post is about the approximate nearest neighbor (ANN) algorithm. The code for this post is here, where I provide an example of using a framework and a python implementation. Most python implementation were written with the help of a LLM. I’m amazed, how helpful they are for learning new things. I see them like a drunken professor, which with the right approach will be a very helpful tool.
As a next step in understanding RAGs, I want to have a closer look at approximate nearest neighbor algorithms. Basically, the purpose is to find the closest vector to a query vector in a database. Since I’m also interested into the implementation, I follow mostly this amazing blog post. Vector search is the basic component of vector databases and their main purpose. ANN algorithms are looking for a close match instead of an exact match. This loss of accuracy allows an improvement of efficieny, which allows the search through much bigger datasets, high-dimensional data and real-time apllications.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-04-19T10:30:58+01:00">
    <meta property="article:modified_time" content="2025-04-19T10:30:58+01:00">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Algorithm">
    <meta property="article:tag" content="Nlp">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Understanding approximate nearest neighbor algorithm">
<meta name="twitter:description" content="Hi,
This post is about the approximate nearest neighbor (ANN) algorithm. The code for this post is here, where I provide an example of using a framework and a python implementation. Most python implementation were written with the help of a LLM. I&rsquo;m amazed, how helpful they are for learning new things. I see them like a drunken professor, which with the right approach will be a very helpful tool.
As a next step in understanding RAGs, I want to have a closer look at approximate nearest neighbor algorithms. Basically, the purpose is to find the closest vector to a query vector in a database. Since I&rsquo;m also interested into the implementation, I follow mostly this amazing blog post. Vector search is the basic component of vector databases and their main purpose. ANN algorithms are looking for a close match instead of an exact match. This loss of accuracy allows an improvement of efficieny, which allows the search through much bigger datasets, high-dimensional data and real-time apllications.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://steffenhaeussler.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Understanding approximate nearest neighbor algorithm",
      "item": "https://steffenhaeussler.github.io/posts/2025-04-19-ann/ann/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Understanding approximate nearest neighbor algorithm",
  "name": "Understanding approximate nearest neighbor algorithm",
  "description": "Hi,\nThis post is about the approximate nearest neighbor (ANN) algorithm. The code for this post is here, where I provide an example of using a framework and a python implementation. Most python implementation were written with the help of a LLM. I\u0026rsquo;m amazed, how helpful they are for learning new things. I see them like a drunken professor, which with the right approach will be a very helpful tool.\nAs a next step in understanding RAGs, I want to have a closer look at approximate nearest neighbor algorithms. Basically, the purpose is to find the closest vector to a query vector in a database. Since I\u0026rsquo;m also interested into the implementation, I follow mostly this amazing blog post. Vector search is the basic component of vector databases and their main purpose. ANN algorithms are looking for a close match instead of an exact match. This loss of accuracy allows an improvement of efficieny, which allows the search through much bigger datasets, high-dimensional data and real-time apllications.\n",
  "keywords": [
    "machine learning", "algorithm", "nlp"
  ],
  "articleBody": "Hi,\nThis post is about the approximate nearest neighbor (ANN) algorithm. The code for this post is here, where I provide an example of using a framework and a python implementation. Most python implementation were written with the help of a LLM. I’m amazed, how helpful they are for learning new things. I see them like a drunken professor, which with the right approach will be a very helpful tool.\nAs a next step in understanding RAGs, I want to have a closer look at approximate nearest neighbor algorithms. Basically, the purpose is to find the closest vector to a query vector in a database. Since I’m also interested into the implementation, I follow mostly this amazing blog post. Vector search is the basic component of vector databases and their main purpose. ANN algorithms are looking for a close match instead of an exact match. This loss of accuracy allows an improvement of efficieny, which allows the search through much bigger datasets, high-dimensional data and real-time apllications.\nThis improved efficiency is based on indexes, which are pre-processed data-structures of embeddings. This also includes dimension reduction of the intitial embedding. There are multiple techniques, how the embedding can be indexed for a fast retrieval. To get a retrieval, there are multiple approaches to calculate the distance between two embeddings (e.g. cosine distance, euclidean distance). To get an extensive list of almost all metrics, please have a look at the NMSLIB manual here.\nFor this post, there will be ashort introduction for:\nhash-based indexing tree-based indexing graph-based indexing cluster-based indexing The ressources for this post are:\nVector Database 101: Everything You Need to Know Faiss: The Missing Manual Gemini 2.5 pro for consulting and coding (honestly, I don’t know, how to cite LLMs or mark the parts, where they helped me) As always, look at the code and the ressources. I’m sparse with graphics, but sometimes they make it much easier to understand the concept.\nGraph-based Indexing (e.g. HNSW) Hierarchical Navigable Small World uses a graph for approximity nearest neighbor search. A “proximity graph” is used, which links two vertices based on their closeness (e.g. euclidean distance). There are two fundamental techniques, which contribute to HNSW. It’s a probability skip list and navigable small world graphs.\nA probability skip list allows fast search, while using a linked list for easy and fast insertion of new elements. This is done by adding multiple layers of linked lists, where intermediate nodes are skipped in the low layers. This allows a fast search by following the skips until the target is found.\nNavigable small world graphs are proximity graphs, which include long-range and short-range links to reduce search times. Each graph has an pre-defined entry-point, which connects to several vertices. Via greedy routing the closest vertice to the search query will selected until a local minimum is detected.\nHNSW combines the hierarchical multi-layers from the skip list with the long-range (sparse) and short-range (dense) links of the NSW graph. Please look at one of the references. With the visual explanation from those blogs, it is much easier to grasp the concept og HNSW.\nRessources:\nHierarchical Navigable Small Worlds Understanding HNSWlib: A Graph-based Library for Fast Approximate Nearest Neighbor Search Skip Lists: A Probabilistic Alternative to Balanced Trees by Pugh 1990 Approximate nearest neighbor algorithm based on navigable small world graphs by Malkov et al. 2014 Cluster-based Indexing (e.g. product quantization) Product quantization is a way to cluster chunks of a vector. This reduces the total size of the database by reducing the overall precision of the vectors. This is different from PCA, which reduce the dimension of a vector. The idea is, that the algorithm looks at each dimension separately and “bins” each value into discrete buckets. The search will performed over the quantized vectors, which results in reduced memory consumption and a speed-up.\nBasically, the vector is split into subvectors. Each subvector is quantized by k-means and will be represented by the index of its closest centroid.\nI look into three different way of PQ\nsimple PQ Inverted File + Product Quantization, which introduces a better data structure to achieve a much denser subdivision of the search space Optimized Product Quantization with minimizing quantization distortions Ressources:\nProduct Quantization Scalar Quantization and Product Quantization Product Quantization for Similarity Search Product Quantization for Nearest Neighbor Search by Jegou et al. 2011 The Inverted Multi-Index by Babenko et al. 2012 Optimized Product Quantization by Ge et al. 2013 Hash-based Indexing (e.g. LSH) Locality sensitive hashing is an approximate technique, which puts similiar data points into same buckets or locations with high probability. For retrieval, you only need to explore a small subset instead of the whole dataset. LSH addresses this by increasing the likelihood of hash value collisions for similiar inputs. By those collisions similiar embeddings will be grouped together.\nLSH works in three major steps:\nShingling (converting the documents into a set of characters of length k) - not needed here, since we already use embeddings Minhashing (calculates similiarity between two data points by assigning Minhash signatures) Locality sensitive hashing (clustering similar items together) Ressources:\nLSH Algorithm and Implementation LSH guide Locality Sensitive Hashing (LSH): The Illustrated Guide Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions, Andoni and Indyk 2006 Approximate Nearest Neighbor with Locality Sensitive Hashing Tree-based Indexing (e.g. Annoy) Approximate Nearest Neighbors Oh Yeah (Annoy) is an algorithm, which uses a forest of trees to search for the nearest neighbor. Annoy is an optimized approach and hard to implement by yourself. To keep in mind, annoy was published by spotify, which was replaced by Voyager. Anyway, traditional tree-based approaches like KD-Trees suffer from the high dimensionality. The sklearn inbuild version of a kd-tree takes 3 sec to get the approx. nearest neighbors for 10 samples. The documentation of annoy is a bit sparse.\nThe core idea is to recursively partition the data. During the build phase, the data is divided at each node based on different criteria (e.g., splitting along a dimension like in kd-trees, partitioning by distance to chosen centers like in ball trees, or splitting based on a random projection like in RP trees). This splitting continues until the nodes contain a certain number of points.\nFor the query phase, the search traverses the tree. Starting at the root, the algorithm uses the node’s splitting criteria to decide which child branch is most likely to contain the nearest neighbors for the given query point.\nRessources:\nImplementing Approximate Nearest Neighbor Search with KD-Trees KD-Tree Random Projection This is a brief overview of approximate nearest neighbor algorithms. I hope, you find it helpful. You can find the code for an example dataset here.\nThank you for your attention.\n",
  "wordCount" : "1116",
  "inLanguage": "en",
  "datePublished": "2025-04-19T10:30:58+01:00",
  "dateModified": "2025-04-19T10:30:58+01:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://steffenhaeussler.github.io/posts/2025-04-19-ann/ann/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "About me",
    "logo": {
      "@type": "ImageObject",
      "url": "https://steffenhaeussler.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://steffenhaeussler.github.io/" accesskey="h" title="About me (Alt + H)">About me</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://steffenhaeussler.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://steffenhaeussler.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://steffenhaeussler.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://steffenhaeussler.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://steffenhaeussler.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Understanding approximate nearest neighbor algorithm
    </h1>
    <div class="post-meta"><span title='2025-04-19 10:30:58 +0100 +0100'>April 19, 2025</span>&nbsp;·&nbsp;6 min

</div>
  </header> 
  <div class="post-content"><p>Hi,</p>
<p>This post is about the approximate nearest neighbor (ANN) algorithm. The code for this post is <a href="https://www.kaggle.com/code/steffenhaeussler/understanding-approx-nearest-neighbor-algorithm">here</a>, where I provide an example of using a framework and a python implementation. Most python implementation were written with the help of a LLM. I&rsquo;m amazed, how helpful they are for learning new things. I see them like a drunken professor, which with the right approach will be a very helpful tool.</p>
<p>As a next step in understanding RAGs, I want to have a closer look at approximate nearest neighbor algorithms. Basically, the purpose is to find the closest vector to a query vector in a database. Since I&rsquo;m also interested into the implementation, I follow mostly this amazing blog post. Vector search is the basic component of vector databases and their main purpose. ANN algorithms are looking for a close match instead of an exact match. This loss of accuracy allows an improvement of efficieny, which allows the search through much bigger datasets, high-dimensional data and real-time apllications.</p>
<p>This improved efficiency is based on indexes, which are pre-processed data-structures of embeddings. This also includes dimension reduction of the intitial embedding. There are multiple techniques, how the embedding can be indexed for a fast retrieval. To get a retrieval, there are multiple approaches to calculate the distance between two embeddings (e.g. cosine distance, euclidean distance). To get an extensive list of almost all metrics, please have a look at the NMSLIB manual <a href="https://github.com/nmslib/nmslib/blob/master/manual/spaces.md">here</a>.</p>
<p>For this post, there will be ashort introduction for:</p>
<ul>
<li>hash-based indexing</li>
<li>tree-based indexing</li>
<li>graph-based indexing</li>
<li>cluster-based indexing</li>
</ul>
<p>The ressources for this post are:</p>
<ul>
<li><a href="https://zilliz.com/learn/introduction-to-unstructured-data">Vector Database 101: Everything You Need to Know</a></li>
<li><a href="https://www.pinecone.io/learn/series/faiss/">Faiss: The Missing Manual</a></li>
<li>Gemini 2.5 pro for consulting and coding (honestly, I don&rsquo;t know, how to cite LLMs or mark the parts, where they helped me)</li>
</ul>
<p>As always, look at the code and the ressources. I&rsquo;m sparse with graphics, but sometimes they make it much easier to understand the concept.</p>
<h2 id="graph-based-indexing-eg-hnsw">Graph-based Indexing (e.g. HNSW)<a hidden class="anchor" aria-hidden="true" href="#graph-based-indexing-eg-hnsw">#</a></h2>
<p>Hierarchical Navigable Small World uses a graph for approximity nearest neighbor search. A &ldquo;proximity graph&rdquo; is used, which links two vertices based on their closeness (e.g. euclidean distance).
There are two fundamental techniques, which contribute to HNSW. It&rsquo;s a probability skip list and navigable small world graphs.</p>
<p>A  probability skip list allows fast search, while using a linked list for easy and fast insertion of new elements. This is done by adding multiple layers of linked lists, where intermediate nodes are skipped in the low layers. This allows a fast search by following the skips until the target is found.</p>
<p>Navigable small world graphs are proximity graphs, which include long-range and short-range links to reduce search times. Each graph has an pre-defined entry-point, which connects to several vertices. Via greedy routing the closest vertice to the search query will selected until a local minimum is detected.</p>
<p>HNSW combines the hierarchical multi-layers from the skip list with the long-range (sparse) and short-range (dense) links of the NSW graph. Please look at one of the references. With the visual explanation from those blogs, it is much easier to grasp the concept og HNSW.</p>
<p>Ressources:</p>
<ul>
<li><a href="https://www.pinecone.io/learn/series/faiss/hnsw/">Hierarchical Navigable Small Worlds</a></li>
<li><a href="https://zilliz.com/learn/learn-hnswlib-graph-based-library-for-fast-anns">Understanding HNSWlib: A Graph-based Library for Fast Approximate Nearest Neighbor Search</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/78973.78977">Skip Lists: A Probabilistic Alternative to Balanced Trees by Pugh 1990</a></li>
<li><a href="https://www.researchgate.net/profile/Yu-Malkov/publication/259126397_Approximate_nearest_neighbor_algorithm_based_on_navigable_small_world_graphs/links/63733c302f4bca7fd06030b8/Approximate-nearest-neighbor-algorithm-based-on-navigable-small-world-graphs.pdf">Approximate nearest neighbor algorithm based on navigable small world graphs by Malkov et al. 2014</a></li>
</ul>
<h2 id="cluster-based-indexing-eg-product-quantization">Cluster-based Indexing (e.g. product quantization)<a hidden class="anchor" aria-hidden="true" href="#cluster-based-indexing-eg-product-quantization">#</a></h2>
<p>Product quantization is a way to cluster chunks of a vector. This reduces the total size of the database by reducing the overall precision of the vectors. This is different from PCA, which reduce the dimension of a vector. The idea is, that the algorithm looks at each dimension separately and &ldquo;bins&rdquo; each value into discrete buckets. The search will performed over the quantized vectors, which results in reduced memory consumption and a speed-up.</p>
<p>Basically, the vector is split into subvectors. Each subvector is quantized by k-means and will be represented by the index of its closest centroid.</p>
<p>I look into three different way of PQ</p>
<ul>
<li>simple PQ</li>
<li>Inverted File + Product Quantization, which introduces a better data structure to achieve a much denser subdivision of the search space</li>
<li>Optimized Product Quantization with minimizing quantization distortions</li>
</ul>
<p>Ressources:</p>
<ul>
<li><a href="https://www.pinecone.io/learn/series/faiss/product-quantization/">Product Quantization</a></li>
<li><a href="https://zilliz.com/learn/scalar-quantization-and-product-quantization/">Scalar Quantization and Product Quantization</a></li>
<li><a href="https://towardsdatascience.com/product-quantization-for-similarity-search-2f1f67c5fddd/">Product Quantization for Similarity Search</a></li>
<li><a href="https://inria.hal.science/inria-00514462/file/jegou_pq_postprint.pdf">Product Quantization for Nearest Neighbor Search by Jegou et al. 2011</a></li>
<li><a href="https://web.archive.org/web/20171108035058id_/http://www.robots.ox.ac.uk/~vilem/cvpr2012.pdf">The Inverted Multi-Index by Babenko et al. 2012</a></li>
<li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2013/11/pami13opq.pdf">Optimized Product Quantization by Ge et al. 2013</a></li>
</ul>
<h2 id="hash-based-indexing-eg-lsh">Hash-based Indexing (e.g. LSH)<a hidden class="anchor" aria-hidden="true" href="#hash-based-indexing-eg-lsh">#</a></h2>
<p>Locality sensitive hashing is an approximate technique, which puts similiar data points into same buckets or locations with high probability. For retrieval, you only need to explore a small subset instead of the whole dataset. LSH addresses this by increasing the likelihood of hash value collisions for similiar inputs. By those collisions similiar embeddings will be grouped together.</p>
<p>LSH works in three major steps:</p>
<ul>
<li>Shingling (converting the documents into a set of characters of length k) - not needed here, since we already use embeddings</li>
<li>Minhashing (calculates similiarity between two data points by assigning Minhash signatures)</li>
<li>Locality sensitive hashing (clustering similar items together)</li>
</ul>
<p>Ressources:</p>
<ul>
<li><a href="https://www.mit.edu/~andoni/LSH/">LSH Algorithm and Implementation</a></li>
<li><a href="https://zilliz.com/learn/Local-Sensitivity-Hashing-A-Comprehensive-Guide">LSH guide</a></li>
<li><a href="https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/">Locality Sensitive Hashing (LSH): The Illustrated Guide</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/1327452.1327494">Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions, Andoni and Indyk 2006</a></li>
<li><a href="https://pyimagesearch.com/2025/01/27/approximate-nearest-neighbor-with-locality-sensitive-hashing-lsh/">Approximate Nearest Neighbor with Locality Sensitive Hashing</a></li>
</ul>
<h2 id="tree-based-indexing-eg-annoy">Tree-based Indexing (e.g. Annoy)<a hidden class="anchor" aria-hidden="true" href="#tree-based-indexing-eg-annoy">#</a></h2>
<p>Approximate Nearest Neighbors Oh Yeah (Annoy) is an algorithm, which uses a forest of trees to search for the nearest neighbor. Annoy is an optimized approach and hard to implement by yourself. To keep in mind, annoy was published by spotify, which was replaced by <a href="https://engineering.atspotify.com/2023/10/introducing-voyager-spotifys-new-nearest-neighbor-search-library/">Voyager</a>.
Anyway, traditional tree-based approaches like KD-Trees suffer from the high dimensionality. The sklearn inbuild version of a kd-tree takes 3 sec to get the approx. nearest neighbors for 10 samples.
The documentation of annoy is a bit sparse.</p>
<p>The core idea is to recursively partition the data. During the build phase, the data is divided at each node based on different criteria (e.g., splitting along a dimension like in kd-trees, partitioning by distance to chosen centers like in ball trees, or splitting based on a random projection like in RP trees). This splitting continues until the nodes contain a certain number of points.</p>
<p>For the query phase, the search traverses the tree. Starting at the root, the algorithm uses the node&rsquo;s splitting criteria to decide which child branch is most likely to contain the nearest neighbors for the given query point.</p>
<p>Ressources:</p>
<ul>
<li><a href="https://pyimagesearch.com/2024/12/23/implementing-approximate-nearest-neighbor-search-with-kd-trees/">Implementing Approximate Nearest Neighbor Search with KD-Trees</a></li>
<li><a href="https://scikit-learn.org/stable/modules/neighbors.html#kd-tree">KD-Tree</a></li>
<li><a href="https://scikit-learn.org/stable/modules/random_projection.html">Random Projection</a></li>
</ul>
<p>This is a brief overview of approximate nearest neighbor algorithms. I hope, you find it helpful. You can find the code for an example dataset <a href="https://www.kaggle.com/code/steffenhaeussler/understanding-approx-nearest-neighbor-algorithm">here</a>.</p>
<p>Thank you for your attention.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://steffenhaeussler.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://steffenhaeussler.github.io/tags/algorithm/">Algorithm</a></li>
      <li><a href="https://steffenhaeussler.github.io/tags/nlp/">Nlp</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://steffenhaeussler.github.io/">About me</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
