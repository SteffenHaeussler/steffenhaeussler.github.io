<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>About me</title>
    <link>https://steffenhaeussler.github.io/</link>
    <description>Recent content on About me</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Aug 2023 10:30:58 +0100</lastBuildDate><atom:link href="https://steffenhaeussler.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Implementing a transformer network from scratch</title>
      <link>https://steffenhaeussler.github.io/posts/transformer/</link>
      <pubDate>Mon, 28 Aug 2023 10:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/posts/transformer/</guid>
      <description>Hi,
This post is about my implementation of an encoder transformer network from scratch as a follow-up of understanding the attention layer together with the colab implementation. I use a simplified dataset, where I don&amp;rsquo;t expect great results. My approach is building something from scratch to understand it in depth. I faced many challenges during my implementation, so I aligned my code to the BertSequenceClassifier from huggingface. My biggest challenge was to get the network to train.</description>
    </item>
    
    <item>
      <title>Implementing a transformer network from scratch</title>
      <link>https://steffenhaeussler.github.io/projects/transformer/</link>
      <pubDate>Mon, 28 Aug 2023 10:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/projects/transformer/</guid>
      <description>Hi,
This post is about my implementation of an encoder transformer network from scratch as a follow-up of understanding the attention layer together with the colab implementation. I use a simplified dataset, where I don&amp;rsquo;t expect great results. My approach is building something from scratch to understand it in depth. I faced many challenges during my implementation, so I aligned my code to the BertSequenceClassifier from huggingface. My biggest challenge was to get the network to train.</description>
    </item>
    
    <item>
      <title>Learning about time-series analysis</title>
      <link>https://steffenhaeussler.github.io/posts/time_series/</link>
      <pubDate>Tue, 15 Aug 2023 00:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/posts/time_series/</guid>
      <description>Hi,
Recently, I had to work on a simple time-series analysis. I performed poorly since I never worked with time-series before. I believe in a deterministic world, and in general, I prefer to find the causality of a specific data behavior prior to a simple way of empiristic modeling. However, I understand the need for time-series analysis as not enough data available, the underlying processes understood, the complexity bearable, or the time/need for a proper process understanding.</description>
    </item>
    
    <item>
      <title>Learning about time-series analysis</title>
      <link>https://steffenhaeussler.github.io/projects/time_series/</link>
      <pubDate>Tue, 15 Aug 2023 00:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/projects/time_series/</guid>
      <description>Hi,
Recently, I had to work on a simple time-series analysis. I performed poorly since I never worked with time-series before. I believe in a deterministic world, and in general, I prefer to find the causality of a specific data behavior prior to a simple way of empiristic modeling. However, I understand the need for time-series analysis as not enough data available, the underlying processes understood, the complexity bearable, or the time/need for a proper process understanding.</description>
    </item>
    
    <item>
      <title>Endpoint validation</title>
      <link>https://steffenhaeussler.github.io/posts/pydantic/</link>
      <pubDate>Mon, 07 Aug 2023 10:30:50 +0200</pubDate>
      
      <guid>https://steffenhaeussler.github.io/posts/pydantic/</guid>
      <description>Hi,
In my previous job, I spent hours debugging internal data transformations to figure out the received data from an external API was faulty. This issue would not appeared with schema validation. My fault was that I trusted the incoming data and didn‚Äôt check for data consistency. Learning from mistakes and saving time, I would set up a small example for JSON validation via Pydantic. FastAPI relies heavily on pydantic and I use it for validating the incoming request and outgoing response.</description>
    </item>
    
    <item>
      <title>Fast data transfer to or from s3</title>
      <link>https://steffenhaeussler.github.io/posts/s3/</link>
      <pubDate>Thu, 27 Apr 2023 10:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/posts/s3/</guid>
      <description>Hi,
This post is an homage to a stackoverflow post copying data from s3. This shared work saved me a lot of time. I believe that individuals who share their work do not receive sufficient recognition.
The problem is that I have multiple Gb of data separated into thousands of files. Those files are selected for download by the semi-automated pipeline for model training. So the number of files to download varies from pipeline run to pipeline run.</description>
    </item>
    
    <item>
      <title>Training a language model from scratch</title>
      <link>https://steffenhaeussler.github.io/posts/llm/</link>
      <pubDate>Sat, 15 Apr 2023 10:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/posts/llm/</guid>
      <description>Hi,
This post is a short overview over a work project, where I trained a language model for invoices. This so-called base model is then fine-tuned for text classification on customer data. Due to data privacy, a non-disclosure agreement, ISO 27001 and SOAP2, I&amp;rsquo;m not allowed to publish any results. Believe me, it works like üöÄ‚ú®ü™ê.
A language model is trained on large amounts of textual data to understand the patterns and structure of language.</description>
    </item>
    
    <item>
      <title>Training a language model from scratch</title>
      <link>https://steffenhaeussler.github.io/projects/llm/</link>
      <pubDate>Sat, 15 Apr 2023 10:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/projects/llm/</guid>
      <description>Hi,
This post is a short overview over a work project, where I trained a language model for invoices. This so-called base model is then fine-tuned for text classification on customer data. Due to data privacy, a non-disclosure agreement, ISO 27001 and SOAP2, I&amp;rsquo;m not allowed to publish any results. Believe me, it works like üöÄ‚ú®ü™ê.
A language model is trained on large amounts of textual data to understand the patterns and structure of language.</description>
    </item>
    
    <item>
      <title>Cookie-cutter Problems</title>
      <link>https://steffenhaeussler.github.io/posts/cookiecutter/</link>
      <pubDate>Mon, 10 Apr 2023 10:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/posts/cookiecutter/</guid>
      <description>Hi,
Recently, I started to put some scripts together and run them against a Kaggle dataset. I decided to train my skills on an unseen dataset. Training keeps me sharp, and I need it to complement my skill set. For the last 2,5 years, I struggled in a small team with NLP problems, where I worked mostly on engineering tasks. My understanding in this area is not where I wanted to be.</description>
    </item>
    
    <item>
      <title>Cookie-cutter Problems</title>
      <link>https://steffenhaeussler.github.io/projects/cookiecutter/</link>
      <pubDate>Mon, 10 Apr 2023 10:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/projects/cookiecutter/</guid>
      <description>Hi,
Recently, I started to put some scripts together and run them against a Kaggle dataset. I decided to train my skills on an unseen dataset. Training keeps me sharp, and I need it to complement my skill set. For the last 2,5 years, I struggled in a small team with NLP problems, where I worked mostly on engineering tasks. My understanding in this area is not where I wanted to be.</description>
    </item>
    
    <item>
      <title>Stratified multi-label split</title>
      <link>https://steffenhaeussler.github.io/posts/multi_label_split/</link>
      <pubDate>Sat, 08 Apr 2023 10:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/posts/multi_label_split/</guid>
      <description>Hi,
This post is a short overview of a stratified multi-label train-test split. Please look at the colab implementation for a step through guide.
Sometimes you step into work problems, which justify a small post. I already saw colleagues struggling to balance the train-test split for multi-label classification. In classification problems, we have often a dataset with an imbalanced number of classes. In general, it is desired to keep the proportions of each label for the train and test sets as observed as in the original dataset.</description>
    </item>
    
    <item>
      <title>Understanding scaled-dot product attention and multi-head attention</title>
      <link>https://steffenhaeussler.github.io/posts/attention_layer/</link>
      <pubDate>Sat, 18 Mar 2023 10:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/posts/attention_layer/</guid>
      <description>Hi,
This post is a summary of my implementation of the scaled-dot product attention and multi-head attention. Please have a look at the colab implementation for a step through guide.
Even though this post is five years too late, the best way of reviving knowledge is to write about it. Transformers are in transforming the world via ChatGPT, Bart, or LLama. The core of the transformer architecture is the self-attention layer.</description>
    </item>
    
    <item>
      <title>The importance of building things by yourself</title>
      <link>https://steffenhaeussler.github.io/posts/first/</link>
      <pubDate>Sun, 05 Mar 2023 23:30:58 +0100</pubDate>
      
      <guid>https://steffenhaeussler.github.io/posts/first/</guid>
      <description>Hi,
In this initial post, I want to draft the development of my FastAPI skeleton.
At the beginning of my career as a Data Scientist, I ran into the typical problem of model deployment to production. In a team of two scientists, I had the chance to write a micro-service with Flask from scratch out of necessity.
My first service followed strongly the example of Miguel Grinbergs great tutorial. The reason was simple, I couldn&amp;rsquo;t write proper code at this time.</description>
    </item>
    
    
  </channel>
</rss>
